<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[网易云音乐评论]]></title>
    <url>%2F2018%2F10%2F18%2F%E7%BD%91%E6%98%93%E4%BA%91%E8%AF%84%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[想爬取遍地情圣的网易云评论区，发现有一堆坑，还好依赖着前人勉强踩过坑获得了成功。用到的知识主要有:requestes,beautifulsoup,PyCryptodome等。首先我们随机点开一首歌，用requests.get()获取其源码，结果我们发现好多信息是经过加密的，我们不能提取到有效的信息，贴个代码演示一下。 &lt;a href="/song?id=${x.id}">&lt;b title="${x.name|escape}{if alia} - (${alia|escape}){/if}">${soil(x.name)}&lt;/b>&lt;/a>{if alia}&lt;span title="${alia|escape}" class="s-fc8"> - (${soil(alia)})&lt;/span>{/if} {if x.mvid>0} &lt;span data-res-id="${x.id}" data-res-action="mv" title="播放mv" class="mv">MV&lt;/span> {/if} &lt;/span> &lt;/div> &lt;/div> &lt;div class="opt hshow"> &lt;a class="u-icn u-icn-81 icn-add" href="javascript:;" title="添加到播放列表" hidefocus="true" data-res-type="18" data-res-id="${x.id}" data-res-action="addto" {if from}data-res-from="${from.fid}" data-res-data="${from.fdata}"{/if}>&lt;/a> &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="fav" class="icn icn-fav" title="收藏">&lt;/span> &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="share" data-res-name="${x.name}" data-res-author="{list x.artists as art}${art.name}{if art_index&lt;x.artists.length-1}/{/if}{/list}" {if x.album}data-res-pic="${x.album.picUrl}"{/if} class="icn icn-share" title="分享">分享&lt;/span> &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="download" class="icn icn-dl" title="下载">&lt;/span> {if canDel} &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="delete" class="icn icn-del" title="删除">删除&lt;/span> {/if} 我们可以清楚的看到，我们需要提取的信息都被加密过。无法通过普通的办法获得我们想要的内容。这时我们有两种思路：1：使用selenium模拟浏览器的操作进行爬取。此方法我们不在这篇文章里讲述，想尝试的同学可以自己尝试。2：鉴于该数据采取ajax，我们通过访问该网址获取其json字符串，直接完成其信息的采集。我们先按思路二进行下去。 首先在加载的一堆数据中排查得到我们需要的网址，如下： Request URL: https://music.163.com/weapi/v1/resource/comments/R_SO_4_531295576?csrf_token= Request Method: POST Status Code: 200 OK Remote Address: 127.0.0.1:1080 Referrer Policy: no-referrer-when-downgrade Accept: */* Accept-Encoding: gzip, deflate, br Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 Connection: keep-alive Content-Length: 482 Content-Type: application/x-www-form-urlencoded Cookie: 过长就不贴了。 Host: music.163.com Origin: https://music.163.com Referer: https://music.163.com/song?id=531295576 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36 Form Ddta: params: +0YydG970lh0RFa4CtT3YQEDyAB+YtGNJ+UbomdqEnY+TemTKHVbA3vJIX3NKgm07UOy+J2sKcSTM0smfpKbOA4Dm8TJcxbWZrVnjeVl/zE3z3PX21iD84D5Qt50oLrZv8Gug+LqS/y1JWBWiq6DZ0ZaZZEG8aYRYF2bwnPxX9v55DxZJmiubjteCc5lMs0S encSecKey: 762617ad7682a61121e378406d57ffd5aede0f3621e18d1584dab1eca7fdf3ca3d842d882111609a4ada5d3c24973450d1a8553ff96641f144a6de81688ae2c0a5798f8c23fa38e16139999692b6c91150b39c25c014ee6a2005e821485c3a6eab822db16f0397be0c54e43993cc34eae70c68739d19d0eb69e7ca86b16b585a 我们可以看出他是一个post请求，通过多次测试我们发现网址中那个数字会有变化。其他则保持不变，这串数字也很简单的可以发现就是这首歌在网易云的id，我们需要先爬取这个id之后才能进行下一步的操作(以热歌榜为例）。上代码说话。 import requests import os import re from requests import ConnectionError from requests import Timeout headers = { "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36" } def get_index(url): try: response = requests.get(url,headers = headers,timeout = 5) if response.status_code == 200: response.encoding = 'utf8' return response.text else: print("Error",response.status_code) except ConnectionError as e: print("Error",e.args) except Timeout as t: print("Error",t.args) def parse_url(html): hot_list = re.findall(r'&lt;ul class="f-hide">&lt;li>&lt;a href="/song\?id=\d*?">.*&lt;/a>&lt;/li>&lt;/ul>',html) hot_list = hot_list[0] hot_music_id = re.findall(r'&lt;li>&lt;a href="/song\?id=(\d*?)">.*?&lt;/a>&lt;/li>',hot_list) hot_music_name = re.findall(r'&lt;li>&lt;a href="/song\?id=\d*?">(.*?)&lt;/a>&lt;/li>',hot_list) return hot_music_name,hot_music_id if __name__ == "__main__": url = "https://music.163.com/discover/toplist?id=3778678" html = get_index(url) if html: hot_music_name,hot_music_id = parse_url(html) 我用正则表达式提取出其id和名字，既然id到了那么下面应该很简单的吧。答案是这样吗？我们继续看： 请求数据里有两个参数，params 和 encSecKey。如果我们只爬取该歌曲的第一页评论（或者说热评）的话，那么我们只要把这两个参数复制然后构造post请求，可以简单的做到这一点。 def get_html(id): data ={ "params": "dPn4YKam1ALLghjJadhyim4G05a1atAbF7ECvh5EGXihbxOV1i+TRS2oNZv+jR8H6nZ0DSoff6PQQ9mEnocZIv8D5ispn9aLDlta+vylq2wUnJQYLR0KkoQFYcXQ+VOvbmJyLK1acsZ/GuCkP+XFbr3h9WtEAXu3sxfn0DKFO06eNMufFrUHTmw7zyVFp64zYAHK4jIvkSTAkkVYqMu2IbwYG4coJ6wScE1DwKyRQlE=", "encSecKey": "ca034db233499f555cd0009cf367f35093381ded33765fb6026c09c333a20b41d87bcd71504fc87159d55f6b2308343b0d443b8c68c575c1b6e6c37c98fdb571e4a0f61beb292ba4847840ba52f4d0f1c3169e3a2492c00a2cc7b4664a5eb63c6c06be9d90b9a0dc0c305aa8204820225ab5aeace9c2d4ee0dd628a5a022cdba" } url = BASE_URL + id + "?csrf_token=" try: response = requests.post(url,headers=headers,data=data) if response.status_code == 200: return response.json() else: print("Error") except ConnectionError as e: print("Error",e.args) def save_parse(name,data): data = data['hotComments'] if not os.path.exists("Comments"): os.mkdir("Comments") file_path = "{}/Hot.txt".format("Comments") with open(file_path,"a",encoding="utf-8") as f: f.write(name + "\n") for contents in data: f.write(str(num) +":"+ contents['content'] + "\n") f.write("\n" + "***************************************************" + "\n") print("Success",name) 但是我们仅仅满足于热评吗？？？网易云的情圣们可能还隐藏在里面，那么我们继续采坑吧。我们点击评论的下一页，通过开发者工具我们可以发现网址丝毫不变，其他也基本不变，那么我们的数据是怎么改变的呢，post传输的数据为罪魁祸首，那么这些post数据是怎么变化的呢，通过不断地寻找我们锁定了一个js脚本，我们发现这串数据来自于它，并且是加密过的（好难受），加密函数如下： !function() { function a(a) { var d, e, b = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789", c = ""; for (d = 0; a > d; d += 1) e = Math.random() * b.length, e = Math.floor(e), c += b.charAt(e); return c } function b(a, b) { var c = CryptoJS.enc.Utf8.parse(b) , d = CryptoJS.enc.Utf8.parse("0102030405060708") , e = CryptoJS.enc.Utf8.parse(a) , f = CryptoJS.AES.encrypt(e, c, { iv: d, mode: CryptoJS.mode.CBC }); return f.toString() } function c(a, b, c) { var d, e; return setMaxDigits(131), d = new RSAKeyPair(b,"",c), e = encryptedString(d, a) } function d(d, e, f, g) { var h = {} , i = a(16); return h.encText = b(d, g), h.encText = b(h.encText, i), h.encSecKey = c(i, e, f), h } function e(a, b, d, e) { var f = {}; return f.encText = c(a + e, b, d), f } window.asrsea = d, window.ecnonasr = e }(); window.asrsea函数就是那个加密的罪魁祸首。也就是上面的d函数. 可以看出它采用CryptoJS加密,我们可以通过PyCryptodome(python3用户），如果你是python2的话，请安装这个pycrypto，pycrypto已经宣布永久停止维护了，所以请移步PyCryptodome吧，https://github.com/Legrandin/pycryptodome。 那么我们是如何发现这个函数的呢，Chrome开发者控制面板–source–点击 Event Listener Breakpoints–勾选XHR–点击重新加载–然后点击 Step over next function call的那个图标，就这样单步调试过去，就能找到那个函数。。然后我们打开Chrome的调试工具，把断点设在12973行我们可以发现上面的参数（格式化js语句点击下面出现的{}标志即可），既然找到了加密函数和相应的参数，那么我们开始用py模仿破解吧，经过多次翻页我们发现只有{rid: “R_SO_4_574566207”, offset: “40”, total: “false”, limit: “20”, csrf_token: “”}{rid: “R_SO_4_574566207”, offset: “60”, total: “false”, limit: “20”, csrf_token: “”}可以看出只有offset参数是变化的，limit参数经过分析是控制每页评论数的，限制每页20条，其他则是固定不变的，结合上面的window.asrsea()函数，我们构造出加密字符串，完成post请求进而得到我们的评论数据。 回到我们的加密函数上，我们发现它经过AES加密和RSA加密。 function d(d, e, f, g) { var h = {} , i = a(16); return h.encText = b(d, g), #AES加密 h.encText = b(h.encText, i), #AES加密 h.encSecKey = c(i, e, f), #RSA加密 h } 不懂这些加密函数什么意思，自学啊，还是先模仿吧。用到我们上面写到的PyCryptodome库，进行Crypto加密。我还是贴代码吧。。 #导入这些第三方库 import requests import math import random import base64 import codecs from Crypto.Cipher import AES 首先我们需要生成长度为16的随机字符串,这里我们仿照上面的javascript的实现,用Python生成16位长的随机字符串: def generate_random_strs(length): string = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789" # 控制次数参数i i = 0 # 初始化随机字符串 random_strs = "" while i &lt; length: e = random.random() * len(string) # 向下取整 e = math.floor(e) random_strs = random_strs + list(string)[e] i = i + 1 return random_strs AES加密的模式是AES.MODE_CBC,初始化向量iv=’0102030405060708′,具体的AES加密 # AES加密 def AESencrypt(msg, key): # 如果不是16的倍数则进行填充(paddiing) padding = 16 - len(msg) % 16 # 这里使用padding对应的单字符进行填充 msg = msg + padding * chr(padding) # 用来加密或者解密的初始向量(必须是16位) iv = '0102030405060708'.encode('utf-8') key = key.encode('utf-8') cipher = AES.new(key, AES.MODE_CBC, iv) # 加密后得到的是bytes类型的数据 encryptedbytes = cipher.encrypt(msg.encode('utf-8')) # 使用Base64进行编码,返回byte字符串 encodestrs = base64.b64encode(encryptedbytes) # 对byte字符串按utf-8进行解码 enctext = encodestrs.decode('utf-8') return enctext RSA加密.首先我简单介绍一下RSA的加密过程.在RSA中,明文,密钥和密文都是数字.RSA的加密过程可以用下列的公式来表达,这个公式非常的重要,你只有理解了这个公式,才能用Python实现RSA加密 密文 = 明文^E mod N (RSA加密) RSA的密文是对代表明文的数字的E次方求mod N 的结果, 通俗的讲就是将明文和自己做E次乘法,然后将其结果除以N 求余数,这个余数就是密文. # RSA加密 def RSAencrypt(randomstrs, key, f): # 随机字符串逆序排列 string = randomstrs[::-1] # 将随机字符串转换成byte类型数据 text = bytes(string, 'utf-8') seckey = int(codecs.encode(text, encoding='hex'), 16)**int(key, 16) % int(f, 16) return format(seckey, 'x').zfill(256) RSA加密后得到的字符串长为256,如果不够长则进行填充(不足部分在左侧添0).然后就是获取那两个参数。 def get_params(page): # msg也可以写成msg = {"offset":"页面偏移量=(页数-1) * 20", "limit":"20"},offset和limit这两个参数必须有(js) # limit最大值为100,当设为100时,获取第二页时,默认前一页是20个评论,也就是说第二页最新评论有80个,有20个是第一页显示的 # msg = '{"rid":"R_SO_4_1302938992","offset":"0","total":"True","limit":"100","csrf_token":""}' # 偏移量 offset = (page-1) * 20 # offset和limit是必选参数,其他参数是可选的,其他参数不影响data数据的生成 msg = '{"offset":' + str(offset) + ',"total":"True","limit":"20","csrf_token":""}' key = '0CoJUm6Qyw8W8jud' f = '00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7' e = '010001' enctext = AESencrypt(msg, key) # 生成长度为16的随机字符串 i = generate_random_strs(16) # 两次AES加密之后得到params的值 encText = AESencrypt(enctext, i) # RSA加密之后得到encSecKey的值 encSecKey = RSAencrypt(i, e, f) return encText, encSecKey 下面构造表单不再赘述，所有参数都已得到。如果想用selenium的大佬，可以自己尝试一下。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k近邻算法]]></title>
    <url>%2F2018%2F10%2F13%2Fk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[k近邻算法，简单的来说就是根据测量不同特征值之间的距离进而对数据集进行分类。该算是原理是根据我们已知的特征值对应关系，对我们想要分类的新数据进行判断分类。比如我们想判断一个电影的题材那么我们就可以提取一定的特征值，电影中惊悚片段的次数等等，然后用该算法构造程序，自动化分。存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 k近邻算法的流程1收集数据：爬虫或者一些已存在的数据集2准备数据：结构化数据格式3分析数据：可用任何方法4训练算法：（不适用于k近邻）5测试算法：计算成功率6使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理 请看代码：1：导入数据 from numpy import * import operator def createDataSet(): group = array([[1,1],[3.1],[1,3],[3,3]) labels = ['A',"B","C","D"] return group,labels 定义一个标准数据集（每组包含两个数据）和标签，一一对应。我们将[1,1]定义为类A,依次类推。 进行距离运算，预测类型 def classify0(inX, dataSet,labels,k): dataSetsize = dataSet.shape[0] #计算训练数据集的行数 diffMat = tile(inX,(dataSetsize,1) - dataSet) sqDiffMat = diffMat**2 sqDistance = sqDiffMat.sum(axis) #将一轴上的数据相加 distance = sqDistance**0.5 sorteDistIndicies = distance.argsort() #返回distances中元素从小到大排序后的索引值 classCount = {} for i in range(k): voteIlable = labels[sorteDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) #key=operator.itemgetter(1)根据字典的值进行排序，reverse = True 以降序排列 return sortedClassCount[0][0] 这样我们就可以返回次数出现最高的标签进而输出预测值，但是我们无法判断这个分类是否正确，因此我们需要测试一下。 以海伦约会为例:她把这些数据存放在文本文件datingTestSet.txt中海伦的样本主要包含以下3种特征： 每年获得的飞行常客里程数 玩视频游戏所耗时间百分比 每周消费的冰淇淋公升数首先py读取收集到的txt文件数据，使其可以被使用 def file2matrix(filename): fr = open(filename) arrayoLine = fr.readlines() numberoflines = len(arrayoLine) returnMat = zeros((numberoflines,3)) classLabelVectors = [] index = 0 for line in arrayoLine: line = line.strip() #去除空白 listFormLine = line.split('\t') #以制表符为标志切片 returnMat[index,:] = listFormLine[0:3] if listFromLine[-1] == 'didntLike': classLabelVector.append(1) elif listFromLine[-1] == 'smallDoses': classLabelVector.append(2) elif listFromLine[-1] == 'largeDoses': classLabelVector.append(3) index+=1 return returnMat,classLabelVector 返回读取成功的数据集，和标签集合集合。先进行图形化分析一下。 import matplotlib import matplotlib.pyplot as plt fig = plt.figure() #创建一个画布 ax = fig.add_subplot(111) ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*array(datingLabels),15.0*array[datingLabels]) #以特征值一和二为数据画图 plt.show() 图略，我们发现一些数字值相差太大影响我们的结果，因此我们进行归一化数值： def autoNorm(dataSet): minVals = dataSet.min(0) #min()中0对应列，1对应行。不输入测全部数据的最小值 maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals,(m,1)) normDataSet = normDataSet/(tile(ranges,(m,1))) return normDataSet,ranges,minVals 将最大的数值当作1，最小的当作0，其他等比例压缩。通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率。本书后续章节还会介绍一些高级方法完成同样的任务，这里我们还是采用最原始的做法。需要注意的是，10%的测试数据应该是随机选择的，由于海伦提供的数据并没有按照特定目的来排序，所以我们可随意选择10%数据而不影响其随机性。 代码里我们定义一个计数器变量，每次分类器错误地分类数据，计数器就加1，程序执行完成之后计数器的结果除以数据点总数即是错误率进入正题测试预测结果： def datingClassTest(): horatio = 0.10 datingDataMat,datingLabels = file2matrix('datingTestSet,txt') normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*horatio) errCount = 0.0 for i in range(numTestVecs): classifreResult = classify0(normMat[i,:],normMat[numTestVecs,:],datingLabels[numTestVecs:m],3) print("分类结果:%s\t真实类别:%d" % (classifierResult, datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print("错误率:%f%%" % (errorCount / float(numTestVecs) * 100)) 一个简单的分类系统就做好了，我们需要优化一下。让海伦找到自己理想的伴侣 def calssifyPerson(): resultList=['not at all','in small doses','in large doses'] percentTats=float(input("percentage of time spent playing video games?")) ffMiles=float(input("frequent flier miles earned per year?")) iceCream=float(input("liters of ice cream consumed per year?")) datingDataMat,datingLabels=file2matrix('datingTestSet2.txt')#原书没有2 normMat, ranges, minVals = autoNorm(datingDataMat) inArr=array([ffMiles,percentTats,iceCream]) classifierResult=classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print("You will probably like this person:", resultList[classifierResult-1]) 我们输入对应的数据，代码帮我们输出他适不适合海伦（错过了真命天子就嘿嘿了） 我们还可以通过k近邻算法，做一个手写识别系统注图像已经经过处理为黑白图像（由0，1）字符组成。 0000000000001111000000000000000000000000000111111000000000000000000000000011111111000000000000000000000000011111111110000000000000000000001111111111110000000000000000000011111111111110000000000000000001111110111111100000000000000000011111000111111000000000000000000111110000111111000000000000000011111000000111111000000000000001111100000001111110000000000000011111000000000111100000000000000111110000000001111100000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000011111100000000011111000000000000011111000000000011110000000000000111111000000000111100000000000001111100000000001111000000000000011111000000000111110000000000000111110000000001111100000000000001111110000000011111000000000000011111100000011111100000000000000011111000000111111000000000000000111111000011111100000000000000000111110000111110000000000000000001111111111111100000000000000000011111111111111000000000000000000011111111111000000000000000000000001111111111000000000000000000000000111110000000000000 将数据转化为测试向量： def img2Vector(filename): returnVect = zeros((1,1024)) fr = open(filenaem) for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int[lineStr[j]] return returnVect 我们将这些数据输入到分类器，检测分类器的执行效果。程序清单2-6所示的自包含函数handwritingClassTest()是测试分类器的代码，将其写入kNN.py文件中。在写入这些代码之前，我们必须确保将from os import listdir写入文件的起始部分，这段代码的主要功能是从os模块中导入函数listdir，它可以列出给定目录的文件名。 import os,sys def handwritingClassTest(): hwLabels=[] trainingFileList=os.listdir('trainingDigits')#修改 import os 这里加上os. m=len(trainingFileList) trainingMat=zeros((m,1024)) #定义文件数x每个向量的训练集 for i in range(m): fileNameStr=trainingFileList[i] fileStr=fileNameStr.split('.')[0]#解析文件 classNumStr=int(fileStr.split('_')[0])#解析文件名 hwLabels.append(classNumStr)#存储类别 trainingMat[i,:]=img2vector('trainingDigits/%s'%fileNameStr) #访问第i个文件内的数据 #测试数据集 testFileList=os.listdir('testDigits') errorCount=0.0 mTest=len(testFileList) for i in range(mTest): fileNameStr=testFileList[i] fileStr=fileNameStr.split('.')[0] classNumStr=int(fileStr.split('_')[0])#从文件名中分离出数字作为基准 vectorUnderTest=img2vector('testDigits/%s'%fileNameStr)#访问第i个文件内的测试数据，不存储类 直接测试 classifierResult=classify0(vectorUnderTest,trainingMat,hwLabels,3) print("the classifier came back with: %d,the real answer is: %d" %(classifierResult,classNumStr)) if(classifierResult!=classNumStr): errorCount+=1.0 print("\nthe total number of errors is: %d" % errorCount) print("\nthe total rate is:%f"% (errorCount/float(mTest))) 两个函数就完成了这个识别系统，代码和数据集在github上都有，这里不在贴。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析库Beautiful-Soup]]></title>
    <url>%2F2018%2F09%2F28%2Fclass2%2F</url>
    <content type="text"><![CDATA[Beautoful Soup借助网页的结构和属性来进行解析网页，比正则表达式要简便的多。省去很多繁琐的提取工作，大大提高了解析效率。1：首先确保我们已经安装了Beautiful Soup 和 lxml. lxml的安装建议不要直接用pip install lxml，会遇到难以消除的错误，从官网直接下载.whl文件（注意和你的Python版本匹配）然后进行安装，亲测方便快捷。 Beautiful Soup 支持四种解析库，我们推荐使用lxml（速度快，容错能力强，可以解析html 和 xml)。话不多说上代码。 如果还未安装的话，可以命令行输入$ pip install beautifulsoup4 from bs4 import BeautifulSoup import re html = """ &lt;html>&lt;head>&lt;title>The Dormouse's story&lt;/title>&lt;/head> &lt;body> &lt;p class="title" name="dromouse">&lt;b>The Dormouse's story&lt;/b>&lt;/p> &lt;p class="story">Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1">&lt;!-- Elsie -->&lt;/a>, &lt;a href="http://example.com/lacie" class="sister" id="link2">Lacie&lt;/a> and &lt;a href="http://example.com/tillie" class="sister" id="link3">Tillie&lt;/a>; and they lived at the bottom of a well.&lt;/p> &lt;p class="story">...&lt;/p> """ soup = BeautifulSoup(html,'lxml') #也可以不适用lxml 库使用默认的解析库 soup = BeautifulSoup(html) #推荐使用lxml 解析 #尝试输出一下 print(soup.prettify()) 发现结果如下，我们的html代码被调整好了格式,不再是那么一坨 &lt;html> &lt;head> &lt;title> The Dormouse's story &lt;/title> &lt;/head> &lt;body> &lt;p class="title" name="dromouse"> &lt;b> The Dormouse's story &lt;/b> &lt;/p> &lt;p class="story"> Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"> &lt;!-- Elsie --> &lt;/a> , &lt;a class="sister" href="http://example.com/lacie" id="link2"> Lacie &lt;/a> and &lt;a class="sister" href="http://example.com/tillie" id="link3"> Tillie &lt;/a> ; and they lived at the bottom of a well. &lt;/p> &lt;p class="story"> ... &lt;/p> &lt;/body> &lt;/html> 这就是我们要介绍的第一个方法 prettify(). 这个方法将需要解析的字符串以标准的格式缩进格式输出，有一点需要注意一下，如果我们原来的字符串没有闭合的话(比如缺少&lt;/a&gt;)经过这个调用会自动更正格式，补全代码（但这一步不是这个函数做的，而是进行初始化时就已经自动完成。）我们学习解析库肯定是要学如何提取信息的啊，那么怎么办呢： 节点选择器-栗子 html 同上面的 print(soup.title) print(soup.title.string) print(soup.p) print(type(soup.title)) >>> &lt;title>The Dormouse's story&lt;/title> The Dormouse's story &lt;p class="title" name="dromouse">&lt;b>The Dormouse's story&lt;/b>&lt;/p> &lt;class 'bs4.element.Tag'> 很直观的看到我们可以通过节点选择器简单的提取信息，选择器解析到的是Tag类型，其中的string属性可以得到节点的文本内容。那么只能提取到文本内容吗，显然不是。 >>>列举一下信息提取方式 print(soup.title.name) #获取节点的名称 print(soup.p.attrs) #可以获取该节点的所有属性，比如id class name等等，它返回一些字典型数据，假如我们只想要其中一个属性那么可以这样 print(soup.p.attrs['name']) 如果你觉得这样很烦，那么可以更直接一点 print(soup.p['name']) ****划重点了 提取出的属性值不一定唯一，name值一般只有一个所以输出为文本类型的，但同一标签里会有很多个class属性，因此class属性提取结果为列表型。 每一步提取出来的节点都是'bs4.element.Tag类型的，也就意味着我们可以二次提取，嵌套操作！ >>> print(soup.head.title) >>>&lt;title>The Dormouse's story&lt;/title> 这一步我们先提取出head 节点，然后再次调用节点选择器，提取出title. 关联选择。网页结构并不是我们理想的那种，有时候我们需要多次选择，从一个基准出发然后选择他的子节点，父节点，兄弟节点（没有姐妹节点）。1——选择节点元素后，如果想直接获得直接子节点我们调用contents属性即可。 （1）子节点 from bs4 import BeautifulSoup html = 原来的html 去除第一个p标签。 soup = BeautifulSoup(html,'lxml') print(soup.p.contents) >>> ['Once upon a time there were three little sisters; and their names were\n', &lt;a class="sister" href="http://example.com/elsie" id="link1">&lt;!-- Elsie -->&lt;/a>, ',\n', &lt;a class="sister" href="http://example.com/lacie" id="link2">Lacie&lt;/a>, ' and\n', &lt;a class="sister" href="http://example.com/tillie" id="link3">Tillie&lt;/a>, ';\nand they lived at the bottom of a well.'] # 可以看出返回结果为列表形式，这里列举的全是直接子节点，假如这些节点内部还有节点（孙节点），也不会单独列出来而是包含在对应的父节点里。 不知你有没有想过为什么叫子节点，是不是有个children属性，答案是肯定的，真的有children属性，我们也可以通过调用它来提取子节点。 >>> print(soup.p.children) >>> &lt;list_iterator object at 0x00000252F0878BE0> 这是一个迭代器，我们可以通过遍历提取出所有的内容。也可以通过list将其转化为列表（但没必要这么做） >>> for i in soup.p.children: print(i) >>> Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1">&lt;!-- Elsie -->&lt;/a> , &lt;a class="sister" href="http://example.com/lacie" id="link2">Lacie&lt;/a> and &lt;a class="sister" href="http://example.com/tillie" id="link3">Tillie&lt;/a> ; and they lived at the bottom of a well. （2）父节点，祖先节点。 如果你想把这个节点下的所有节点都列出来（子节点，子孙节点。。。。。）那么你可以调用descendants属性，同样这也是一个迭代器类. 那么父节点相信你们也能猜一下喽，parents属性就可以调用，不在贴代码，自己实践一下吧。既然有子孙节点，那么祖先节点也一定存在（不要问为什么没有爷爷节点）对应的属性为parents. 说完了先辈和后代，当然不能忘记同辈，我们还有兄弟节点需要了解一下。ps____这里不在赘述代码。 （3）兄弟节点 print(soup.p.next_sibling) print(soup.p.prevoius_sibing) print(list(soup.p.prevoius_sibings)) print(list(soup.p.next_siblings)) 分别可以得到你选择节点的下一个兄弟元素，上一个兄弟元素，以及前面所有兄弟元素的生成器，后面所有元素的生成器。 （4）提取信息 前面已经讲了关联节点的选择方法，既然选择到了对应的节点那么如何获取对应的信息呢，我们的目的是提取信息的呀！ html = """ &lt;html> &lt;body> &lt; p class='story'> once upon a time &lt;a href="http://www.baidu.com" class ="sister" id = 'link1'>Bob&lt;/a>&lt;a href='http://127.0.0.1:5000' class ='brother' id = 'link2'>Lacie&lt;/a> &lt;/p> """ soup = BeautifulSoup(j=html,'lxml') print(type(soup.a.next_sibling)) print(soup.a.next_sibling) print(soup.a.next_sibling.string) # 选取的生成器元素 print(list(soup.a.next_siblings)[0]) # 获取特定节点 print(list(soup.a.next_siblings)[0].attr['class']) print(list(soup.a.next_siblings)[0]['class']) #获取节点的class属性 和我们刚开始讲的方法是一样的！ 方法选择器.上面的选择方法是通过属性选择，速度比较快，但明显不够灵活，还好BeautifulSoup 为我们提供了类似于正则表达式的一种查询方法find_all() find()等，输入对应的参数就可以灵活查询。1：find_all() 和正则的findall()功能类似，但需要的不再是正则表达式，而是一些属性或者文本进行查询操作。 (1)name 故名思意通过节点的名称来查询 它的标准形式是find_all(name,attrs,recursive,text,**kwargs) html = """ &lt;div class = 'A'> &lt;div class = 'A_B'> &lt;h4> hello &lt;/h4> &lt;/div> &lt;div class="body"> &lt;ul class='list' id = 'list-1'> &lt;li class='element'>A&lt;/li> &lt;li class='element'>B&lt;/li> &lt;li class='element'>C&lt;/li> &lt;/ul> &lt;ul class='list list-small' id = 'list-2'> &lt;li class='element'>A&lt;/li> &lt;li class='element'>B&lt;/li> &lt;/ul> &lt;/div> &lt;/div> """ soup = BeautifulSoup(html,'lxml') print(soup.find_all(name = 'ul')) print(type(soup.find_all(name='ul')[0])) >>> [&lt;ul class="list" id="list-1"> &lt;li class="element">A&lt;/li> &lt;li class="element">B&lt;/li> &lt;li class="element">C&lt;/li> &lt;/ul>, &lt;ul class="list list-small" id="list-2"> &lt;li class="element">A&lt;/li> &lt;li class="element">B&lt;/li> &lt;/ul>] &lt;class 'bs4.element.Tag'> 通过返回结果我们可以清晰的看到返回的是列表形式，其中每一个均为bs4.element.Tag类型。 既然是这种类型那么显然我们可以嵌套查询 for ul in soup.find_all(name='ul'): print(ul.find_all(name='li')) >>> [&lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>, &lt;li class="element">C&lt;/li>] [&lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>] 那么我们要获取每个li节点的信息也就顺理成章的可以写成 for ul in soup.find_all(name='ul'): for li in ul.find_all(name = 'li'): print(li.string) (2) attrs 我们也可以输入属性查询例如： print(soup.find_all(attrs={'id': 'list-1'})) 该参数为字典类型attrs={'**': "***"} 对于一些常用的属性比如id class 我们可以不用attrs传递（可能是因为懒吧）我们可以这么做： print(soup.find_all(id = 'list-1')) print(soup.find_all(class_= 'element')) >>> [&lt;ul class="list" id="list-1"> &lt;li class="element">A&lt;/li> &lt;li class="element">B&lt;/li> &lt;li class="element">C&lt;/li> &lt;/ul>] [&lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>, &lt;li class="element">C&lt;/li>, &lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>] 可能有人要问为什么class要加下划线，因为class在Python里是关键字，我们这里需要区分一下。 (3)text 匹配节点的文本，传入的可以是字符串，或者正则表达式（有点抢饭碗的意思） import re print(soup.find_all(text = re.compile('A'))) print(soup.find_all(text ='A')) 返回所有匹配正则表达式的节点文本组成的列表。 2-find() 返回单个元素，其他用法和find_all()相同还有一些方法，比如说 find_parents() /find_parent() #返回祖先节点 /返回父节点 find_next_siblings() / find_next_siblings #返回后面所有兄弟节点/返回后面的第一个兄弟节点 find_previous_siblings() / find_previous_sibling() #返回前面所有兄弟节点/返回前面第一个兄弟节点 find_all_next / find_next #返回节点后所有符合条件的节点/ 返回第一个 find_all_previous() / find_previous() #返回节点前所有符合条件的节点/ 返回第一个 CSS选择器如果对web熟悉的话那么CSS选择器你肯定很熟悉，不熟悉的话可以找一下教程。使用CSS选择器我们需要调用select()方法，传入相应的CSS选择器即可 print(soup.select('.A .A_B')) print(soup.select('ul li')) print(soup.select("#list-1 .element")) # 支持嵌套选择 for ul in soup.select('ul'): print(ul.select('li')) # 获取属性方法同上面所讲不在赘述 # 获取文本多加了一个方法get_text() for li in soup.select('li'): print(li.get_text()) print(li.string) o文明k，大概就这么多东西。推荐lxml库，节点选择器速度快但是很繁琐。建议使用find_all(),如果对CSS选择器熟悉的话，推荐select()方法选择。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests 库 and 正则表达式]]></title>
    <url>%2F2018%2F09%2F21%2Fclass1%2F</url>
    <content type="text"><![CDATA[简单的讲一下requests 请求库首先确定你已经安装了requests库，如果没有的话 pip install requests 请求可以这样写 $ r = requests.get("http://www.baidu.com") #对应GET $ data = {'key':'value'} $ r = requests.post("http://www.baidu.com",data=data) #对应POST 提交表单, data为表单数据，以字典型数据存储 $ #主要前两种后面几乎用不到 $ r = requests.put("http://www.baidu.com") #对应PUT $ r = requests.delete("http://www.baidu.com") #对应删除DELETE $ r = requests.head("http://www.baidu.com") # 很方便的看出请求很简单，一行代码就可以完成请求，但是这只是一般网站的主页，我们肯定是要去其他部分提取信息的，所以会附加一大串的参数，比如 r = requests.get('http://httpbin.org/get?name=germey&amp;age=22') 虽然也能完成任务，但翻页操作什么的每次都构造这个显然是不够好用，这个库当然不会让我们用这么笨的方法 for age in range(1,50): parmas = { "name" : "germey", "age" : age } r = requests.get( r = requests.get('http://httpbin.org/get',parmas = parmas) 成功完成附加参数的提交 我们还可能遇到一个最low的反爬措施（知乎） 通过User-Agent 浏览器标识来反爬虫，这个我们可以构造headers轻易的破解，还有一些需要登陆才能操作的网站同样也可以通过构造headers headers = { 'Cookie' : 'ABTEST=0|1534834504|v1; SNUID=1C3DF36DB3B6C05E33DD8DD8B303CE03; IPLOC=CN3201; SUID=AF8E41DF3E18960A000000005B7BB748; SUID=AF8E41DF2C18960A000000005B7BB748; SUV=00151DF1DF418EAF5B7BB74C32B4D814; weixinIndexVisited=1; ppinf=5|1534908615|1536118215|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZTo2OlBzeWNob3xjcnQ6MTA6MTUzNDkwODYxNXxyZWZuaWNrOjY6UHN5Y2hvfHVzZXJpZDo0NDpvOXQybHVJV0FROGwzSTFjYlg3M3Z1akxfd3prQHdlaXhpbi5zb2h1LmNvbXw; pprdig=xm9mkmMYDlvMRbH0pAjDEpcsEDCvoz3ORcB-9-lzvVhkxyM55AmN7NQJ8KU3Ei67B6DAqmo_DjyIu3NchvKgznUCthv3eMG2u_T1MhiMkJD7nV3HrDRKv0KVeNyQnt4Zl6D4y1v8SlHfHd-6aGhSCKW_NDIv_JqJmP-7eWQgsKw; sgid=22-36710215-AVt82MciatqU4SaMibG2iceyUs; sct=3; ppmdig=1534925103000000f514b9c5b91510b7cd851626696ec141; JSESSIONID=aaaGoZeH3bNXx-s9OFBvw; SL_GWPT_Show_Hide_tmp=1; SL_wptGlobTipTmp=1', 'Host' : 'weixin.sogou.com', 'Upgrade-Insecure-Requests' : '1', 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36' } requests,get('http://weixin.sogou.com',headers = headers) 当然也可以cookies = { 'a' : 'a', } requests,get('http://weixin.sogou.com',cookies = cookies) 但显然直接复制更方便 如果遇到反爬措施怎么更换代理呢，requests也有十分简单的方法，类似于上面的参数提交 proxies = { "http" : "http:127.0.0.1:8080", "https" : "https://127.0.0.0:5555" } r = requests.get('http:www.zhihu.com',proxies=proxies) #当然上面的代理是不能用的，你可以找国内哪些免费网站上寻找，或者用后续的维护一个代理池，当然付费代理最为好用。 还有一个超时操作的处理直接在末尾的timeout 参数赋值即可，如果你使用的是GET、OPTIONS、POST、PUT、PATCH 或者 DELETE，那么你可以通过 allow_redirects 参数禁用重定向处理： r = requests.get('http://github.com', allow_redirects=False) 遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常。 如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常。 若请求超时，则抛出一个 Timeout 异常。 若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。 所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。我们通过上面的设置，利用try except 函数可以容易的捕获这些错误此外还有一些证书认证操作，比如12306,他们的证书没有被官方认证，所以爬取会报错，这个设置也很简单。把 verify 参数设置为False 即可，不过会提醒我们一个警告，我们可以选择忽略他， import requests from requests.packages import urllib3 urllib3.disable_warnings() response = requests.get('https//www.12306.cn',verify=False) 讲完了请求那么我们获取的数据怎么拿到呢。 r.status_code #获取响应码，判断自己是否成功访问网站 r.text #将源码转换为text格式 r.encoding = 'utf-8' # 改变网页源码的编码格式，一般是gbk,或者utf-8 r.url #获取请求的网址 type(**) #输出内容的格式，比如text,json ,dict r.json() #可以发现，调用json(）方法，就可以将返回结果是 JSON 格式的字符串转化为字典dict r.content #图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式， 我们才可以看到这些形形色色的多媒体 所以，想要抓取它们，就要拿到它们的二进制码 正则repip install re 正则表达式是处理字符串的强大工具，它有向己特定的语法结构，有了它，实现字符串的检索、替换、匹配验证都不在话下当然,对于爬虫来说，有了它，从HTML 里提取想要的信息就非常方便了可以在开源中国里练手，强大的在线匹配功能http://tool.oschina.net/regex/正则不是python独有的，其他编程语言中也有，re库是整个正则表达式的实现，很方便实用。 match()这个函数可以检测你的正则表达式是否能成功匹配数据，如果失败就返回None,代表匹配失败。举个栗子！ re.match(pattern, string, flags=0) #对应正则表达式，字符串，标识符 栗子： import re str = "Hello 123 456 World is a demo" print(len(str)) result = re.match('^Hello\s\d\d\d\s\d{3}\s\w{5}',str) print(result) print(result.span()) print(result.group()) 我们的正则表达式以^作为开头，然后\s匹配空白字符，\d匹配数字,\d{3}代表匹配三个数字，\w{5},匹配字母数字或下划线共五个。我们运行一下发现匹配到了 Hello 123 456 World，调用group()获取匹配内容，span()获取匹配范围。match()函数有两个参数，1为正则，二为待匹配字符串。如果我们想要得到某一具体内容怎么办呢，比如只想得到123。那么我们可以在正则表达式中用()把它括起来 result = re.match('^Hello\s(\d+)\s\d{3}\sWorld',str) print(result.group(1)) 这样难免有点繁琐，我们使用通用匹配就很方便——.(.可匹配除换行符之外的任意字符。匹配无限次)完成上面的匹配，我们可以这样写 result = re.match('^Hello.*demo',str) print(result.group()) 那么这样真的没有缺点吗，答案是否定的：比如你想得到123这个数字 result = re.match('^Hello.*(\d+).*demo'，str) print(result.group(1)) 运行一下发现结果不如人意并不是我们想要的 这里就涉及到贪婪和非贪婪的问题，.会匹配尽可能多的字符（贪婪匹配），我们只需要简单的更改就能成功(加？).? 是非贪婪匹配，匹配尽可能少的字符，到数字的前面时就默认停止匹配，从而让我们得到正确的结果但这里需要注意，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它匹配尽可能少的字符 例如： import re content = 'http://weibo.com/comment/KEraCN' result1 = re.match('http.*?comment/(.*?)',content) result2 = re.match('http.*?comment/(.*)',content) print("result1",result1.group(1)) print('result2', result2.group(1)) >>> result1 result2 KEraCN 修饰符正则表达式可以包含一些可选标志修饰符来控制匹配的模式 修饰符被指定为一个可选的标志。我们改一下字符串 str = "Hello 123 456 World is a demo" result = re.match('^Hello.*demo',str) print(result.group()) 发现报错，未成功得到字符 这是因为．匹配的是除换行符之外的任意字符，当遇到换行符时，.*?就不能匹配了，所以导致匹配失败这里只需加一个修饰符 re.S ，即可修正这个错误： result = re.match('^Hello.*demo',str,re.S) re.I #使匹配大小写不敏感 re.S #使.匹配包括换行符在内的所有字符 还有其他不常用，自己去查阅吧 那么如果我们要匹配的字符包含.或者其他的特殊字符怎么办呢，比如匹配网址http://www.baidu.com。 这个时候就要用到强大的转义字符了 content = '(百度）www.baidu.com' result = re.match('\(百度\)www\.baidu\.com',content) print(result) search()match()方法是从字符串的开头匹配，我们用它来提取信息显然不方便，更适合用来检测某个字符串是否符合规则，所以我们介绍search()-扫描整个字符串返回第一个成功匹配的结果，没找到则返回None. re.search(pattern, string, flags=0) #对应正则表达式，字符串，标识符 栗子： import re html = '''&lt;div id = 'song-list'> &lt;h2 class = 'title'>经典老歌&lt;/h2> &lt;p class ='introduction'> 经典老歌列表&lt;/p> &lt;ul id ='list' class = 'list-gruop'> &lt;li data-view ='2'>一路有你&lt;/li> &lt;li data-view='7'> &lt;a href='/2.mp3' singer='任贤齐'>沧海一声笑&lt;/a> &lt;/li> &lt;li data-view = '4' class ='active'> &lt;a href='/3.mp3' singer='齐秦'>往事随风&lt;/a> &lt;/li> &lt;li data-view='6'>&lt;a href='/4.mp3' singer = 'beyond'>光辉岁月&lt;/a>&lt;/li> &lt;li data-view='5'>&lt;a href='/5.mp3' singer = '陈慧琳'>记事本&lt;/a>&lt;/li> &lt;li data-view='5' &lt;a href ='/6.mp3' singer='邓丽君'>但愿人长久&lt;/a> &lt;/li> &lt;/ul> &lt;/div>''' result = re,search('&lt;li.*?active.*?singer='(.*?)'>(.*?)&lt;/a>',html,re.S) print(result.group(1),result.group(2)) >>>齐秦往事随风 result = re,search('&lt;li.*?singer='(.*?)'>(.*?)&lt;/a>',html,re.S) print(result.group(1),result.group(2)) >>>任贤齐沧海一声笑 假如我们去掉换行符re.S result = re,search('&lt;li.*?singer='(.*?)'>(.*?)&lt;/a>',html) print(result.group(1),result.group(2)) >>>beyond光辉岁月 findall()search方法只能返回一个值，显然不足以满足我们的需求，findall方法返回所有符合要求的字符串，以列表的形式存储。 re.findall(pattern, string, flags=0) html 同上 results = re.findall('&lt;li.*?href='(.*?)'.*?singer='(.*?)'>(.*?)&lt;/a>',html,re.S) for result in results: print(result) print(result[0],result[1],result[2]) >>>自己运行吧 sub()除了提取信息我们还可用用正则修改文本，这时候就用到了sub()方法。还以上面的为例 >>>html = re.sub('&lt;a.*?>|&lt;/a>','',html) results = re.findall('&lt;li.*?>(.*?)&lt;/li>',html,re.S) for result in results: print(result.strip()) >>>自己运行吧 compile()前面所讲的方法都是用来处理字符串的方法，最后再介绍一下 compile()方法，这个方法可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用 import re contnet = '2018-2-11 12:00' pattrtn = re.compile('\d{2}:\d{2}') 另外， compile()还可以传入修饰符，例如 re.S等修饰符，这样search(),findall()等方法中就不需要额外传了 所以compile()方法可以说是给正则表达式做了一层封装，以使我们更好地复用 ok 扯淡完毕本节结束具体看实例吧]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬微信搜狗文章]]></title>
    <url>%2F2018%2F09%2F01%2Fweixin%2F</url>
    <content type="text"><![CDATA[这个爬虫用到了代理池，Pyquery解析库，requests请求库,and 万年不变的MongoDB数据库.完成这个爬虫我们需要做下列准备，首先当然是分析一下网页的结构，我们可以随便搜索一下，发现如果不登录可以看前十页内容，登陆后可以看到100页，而且每页出现一堆文章列表，打开开发者模式发现他们都各自包含一个超链接，指向微信文章。原来搜狗已经用自己的爬虫爬取了一次文章，然后打包为自己的搜索内容，然而我们多次刷新后会出现验证界面，也就是搜狗的反爬虫机制，这也是我们开头说要用代理的原因，原理基本就是这样，然后贴代码。 首先是设置heads,也就是模拟登陆，网站采取cookies来验证是否登陆，因此我们只需把这些复制过去， headers = { 'Cookie' : 'ABTEST=0|1534834504|v1; SNUID=1C3DF36DB3B6C05E33DD8DD8B303CE03; IPLOC=CN3201; SUID=AF8E41DF3E18960A000000005B7BB748; SUID=AF8E41DF2C18960A000000005B7BB748; SUV=00151DF1DF418EAF5B7BB74C32B4D814; weixinIndexVisited=1; ppinf=5|1534908615|1536118215|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZTo2OlBzeWNob3xjcnQ6MTA6MTUzNDkwODYxNXxyZWZuaWNrOjY6UHN5Y2hvfHVzZXJpZDo0NDpvOXQybHVJV0FROGwzSTFjYlg3M3Z1akxfd3prQHdlaXhpbi5zb2h1LmNvbXw; pprdig=xm9mkmMYDlvMRbH0pAjDEpcsEDCvoz3ORcB-9-lzvVhkxyM55AmN7NQJ8KU3Ei67B6DAqmo_DjyIu3NchvKgznUCthv3eMG2u_T1MhiMkJD7nV3HrDRKv0KVeNyQnt4Zl6D4y1v8SlHfHd-6aGhSCKW_NDIv_JqJmP-7eWQgsKw; sgid=22-36710215-AVt82MciatqU4SaMibG2iceyUs; sct=3; ppmdig=1534925103000000f514b9c5b91510b7cd851626696ec141; JSESSIONID=aaaGoZeH3bNXx-s9OFBvw; SL_GWPT_Show_Hide_tmp=1; SL_wptGlobTipTmp=1', 'Host' : 'weixin.sogou.com', 'Upgrade-Insecure-Requests' : '1', 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36' } 当然cookies会过期复制这个是没用的。然后就是请求网页。 $ global PROXY try: if PROXY: proxies = { 'http' : 'http://' + PROXY } response = requests.get(url = url, allow_redirects = False, headers = headers , proxies = proxies, timeout = 1) else: response = requests.get(url = url, allow_redirects = False, headers = headers, timeout = 1) if response.status_code == 200: return response.text if response.status_code == 302: print("302") proxy = get_proxy() if proxy: print('Using Proxy', proxy) return get_html(url) else: print("Get Proxy Failed") return None except ConnectionError as e: print("Error Occurred", e.args) proxy = get_proxy() return get_html(url,count) except Timeout as T: print("Error Occurred", T.args) proxy = get_proxy() return get_html(url, count) 这里我们获取搜狗的搜索结果页面，如果正常的话返回网页源码，如果触发反爬虫机制，那么我们就选择更换代理，当然我们可以加一个最大重复次数的验证，避免死循环。 def parse_index(html): doc = pq(html) items = doc('.news-box .news-list li .txt-box h3 a').items() for item in items: yield item.attr('href') 我们解析这个网页，然后提取出对应的href，超链接然后通过生成器yield将结果返回上层，再之后就是普通的挨个访问，然后提取信息，这里不在赘述。有一个问题就是pyquery好像不能把时间那个标签完整的提取出来，大佬们可以采取正则表达式尝试一下 from lxml.etree import XMLSyntaxError 在解析过程中，我们发现程序报错显示XMLSyntaxError，这是一个不常见的解析错误，我们将其引入即可，数据库操作还是老样子也不在多提，代理池的事情下篇文章再讲~~~~]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装Gogs的一些坑和心得]]></title>
    <url>%2F2018%2F08%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Gogs作为用go语言写的轻量级的git仓库很适合大家部署在自己的服务器上，建立私人仓库更是很舒服（觉得自己代码不够漂亮）废话不多说进入正题。首先我们要安装数据库：所有的版本都支持 MySQL、PostgreSQL、MSSQL 和 TiDB（使用 MySQL 协议）作为数据库，并且均使用构建标签（build tags）cert 进行构建。需要注意的是，不同的版本的支持状态有所不同，请根据实际的 Gogs 提示进行操作。。我选择的是mysql 代码很简单——sudo apt-get install mysql-server 根据提示一步步安装，完成后可以终端输入 mysql --version 成功的话会看到mysql的版本号 第二步安装git--sudo apt-get install git 同样根据提示进行安装或者 sudo apt-get -y install git （默认所有需要输入操作输入yes） git --version查看版本号观察是否安装成功 上面都成功之后开始创建一个新用户git （是系统用户和git软件并无联系）我们将gogs安装在git用户中 sudo adduser git 按照提示设置密码等 切换git用户 sudo su - git 先安装go语言，linux可直接用sudo apt-get install golang (直接打go是没有的会显示找不到软件包，go又名golang,镜像源中go的包为golang) 附注：如出现找不到软件包可以尝试更新镜像源，国内较好的镜像源有阿里云等，我们可以通过 software-properties-gtk 然后选择合适的镜像源也可以通过编辑源文件——vim /etc/apt/sources.list 亦可以sudo gedit /etc/apt/source.list 然后把自己网上找的镜像源代码复制进去 比如这个网站里的一个源http://blog.sina.com.cn/s/blog_6bc5571a0101077t.html 还可以通过wget 命令直接从网上下载压缩包例如 su - git wget https://dl.google.com/go/go1.9.2.linux-amd64.tar.gz 网址可以自己在网页上寻找最新版的压缩包 解压并删除该安装包（建议先不删避免出现错误之后重新下载） tar -xf go1.9.2.linux-amd64.tar.gz rm -f go1.9.2.linux-amd64.tar.gz tar.gz为二进制压缩文件解压后相当于安装在了该计算机上 我们需要设置环境变量设置 GOROOT 和 GOPATH 目录到系统环境，这样，我们就可以在 git 用户下执行 go 命令。 执行下方的命令 cd ~/ echo 'export GOROOT=$HOME/local/go' >> $HOME/.bashrc echo 'export GOPATH=$HOME/go' >> $HOME/.bashrc echo 'export PATH=$PATH:$GOROOT/bin:$GOPATH/bin' >> $HOME/.bashrc 然后运行source ~/.bashrc 重载Bash 一定确保自己不会写错！认真认真认真！！！！！重要的事情说三遍 大体上完成了我们可以进入正题了！！！！ 使用git安装gogs—–即使用git用户用go命令下载Gogs $ su - git $ go get -u github.com/gogits/gogs 此命令将在 GOPATH/src 目录下载 Gogs 的所有源代码。 个人觉得这个非常慢，刚开始弄甚至以为电脑坏了。。 切换至 $GOPATH/src/github.com/gogits/gogs 目录，并且使用下列命令搭建 Gogs。 cd $GOPATH/src/github.com/gogits/gogs go build 官网上还可以构造分支什么的可以尝试一下。。。不做论述 既然这个比较慢那么当然有快捷办法——参考安装go的方法我们可以运行wget wget -c https://dl.gogs.io/0.11.19/linux_amd64.tar.gz 等待下载完成tar zxvf 压缩包名即可将二进制的文件安装进行下列操作 两种方法完成后都可以进入gogs文件夹运行 ./gogs web 看到这里恭喜你，基本完成了。我们可以crtl c让进程停下。。因为我们的数据库还没有配置 mysql -u root(这是用户名） -p （因为我们要创建用户所以要用最高权限的用户） 输入密码进入然后然后数据库的基本操作百度吧。。（数据库名和数据库用户名不是一个概念） exit; 退出数据库 再次进入gogs文件夹 ./gogs web 浏览器输入对应网址。。。。。不知道本机ip?那么 ipconfig -a 即可看到自己的ip 我的是：http://192.168.176.129:3000/ 然后设置对应的配置 如果还想要其他的操作那么去这里https://linux.cn/article-9391-1.html 以及一些解压文件https://blog.csdn.net/zhongnanjun_3/article/details/6312678 还有vim编译器的读写操作https://my.oschina.net/liujinofhome/blog/36648 同时因为电脑的不同的不同可能还会有其他各种奇葩错误，且行且珍惜]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>gogs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[饿了么爬虫尝试]]></title>
    <url>%2F2018%2F08%2F29%2Felm%2F</url>
    <content type="text"><![CDATA[突发奇想，想爬取外卖商家的信息（虽然吃了一暑假外卖，要吐的感觉）。用到了分析Ajax请求，储存库依旧是MongoDB，还用了一下代理池（因为第一次爬取出现了429状态码，限制爬取速度，但是配置完代理池之后这个错误竟然消失了）headers = { "cookie": "ubt_ssid=rp2q5djs0u6ewmuztcx8mgd2mrkv0vgy_2018-08-29; _utrace=04d8824d7caede2317d2342af2517ebf_2018-08-29; SL_GWPT_Show_Hide_tmp=1; SL_wptGlobTipTmp=1; track_id=1535537453|22aa0399855feb682fdaec9615fcab851e46acb633c7ede4a1|28e66234a33c368946d7476ac4a530f8; USERID=457663306; SID=8qvMlmIw0lzs0oBSvJXJoHiG0ohT7yC77chA", "referer": "https://www.ele.me/place/wtswdqsgbhhq?latitude=32.11645&amp;longitude=118.933531", "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36", "x-shard": "loc=118.933531,32.1164" } 因为需要登陆后才能看到商家信息，所以我们在heads中加入cookies，User-agent参数则是屏蔽爬虫的标识让网站认为我们是正常的浏览。通过观察我们发现网站采取Ajax,和微信与头条一样，下拉能看到更多的内容，因此我们直接爬去这部分字符串即可 BASE_URL = 'https://www.ele.me/restapi/shopping/restaurants?' params = { "extras[]": "activities", "geohash": "wtswdqsgbhhq", "latitude": 32.11645, "limit": 24, "longitude": 118.933531, "offset": offset, "terminal": "web" } url = BASE_URL + urlencode(params) 通过urlencode将文本拼接为正常的url 参数我们可以直接打印出来然后访问这个url 发现返回的是一堆字符串，当然我们要的信息就在里面 response = requests.get(url=url, headers=headers,timeout = 1) if response.status_code == 200: return response.json() timeout 为超时处理，如果超过指定时间未响应则报错得到字符串列表之后通过查找则可以找到相关的信息 for item in json: yield { "shopname" : item.get('name'), "adress" : item.get('address'), "opening_hours" : item.get('opening_hours'), "flavors" : item.get('flavors'), "piecewise_agent_fee" : item.get('piecewise_agent_fee').get('description'), "order_lead_time" : item.get('order_lead_time'), } 分别为店家名称，地址，开放时间，特色，配送费，平均配送时间。 之后储存全套服务不在啰嗦。 还剩下刚开始说的代理池，虽然没太大用，但为了避免时不时的429状态码，还是加上为妙。 def get_proxy(): try: response = requests.get(PROXY_POOL_URL) if response.status_code == 200: return response.text return None except ConnectionError: return None 运用代理则为 if PROXY: proxies = { "https" : "https://" + PROXY } print("Using proxy",PROXY) response = requests.get(url = url,headers = headers,allow_redirects = False,proxies = proxies,timeout = 1) proxies 就是requests包中的代理参数，allow_redirects是禁止重定向，requests函数遇到错误会自动帮我们重定向，因为我们要用代理所以禁止重定向。 基本这个爬虫就完成了，说说它的缺点：这个界面没有详细的餐点目录仅有一些店家的信息，还需多多努力，且行且珍惜]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单的爬虫]]></title>
    <url>%2F2018%2F08%2F29%2Fpyquery%2F</url>
    <content type="text"><![CDATA[简单的说一下用到的知识，多进程，pyquery解析，pymongo链接数据库，requests发送请求这里以一个简单的实例讲解一下如何提取网页信息并储存在MongoDB数据库爬取的网站是https://isujin.com主函数很简单一层一层的传递 def main(page): urls = get_index(page) for url in urls: html = get_html(url) if html: data = parse_html(html) if data: save_as_text(html) save_to_mongo(data)# 如果没安装mongo可以取消此项 save_picture(html) 首先是获取所有的链接 def get_index(page): URL = 'https://isujin.com/page/{}'.format(page) try: response = requests.get(URL) if response.status_code == 200: html = response.text doc = pq(html) urls = doc('#primary .post > a').items() for url in urls: url = url.attr('href') yield url return doc('#post0 > h2 > a').attr('href') except ConnectionError: return None 我们提取出所有的网页的url 如果返回值的状态码为200证明为正确响应，通过pq(html)将HTML网页转化为pyquery的格式，pyquery依据CSS选择器提取信息，#开头根据标签的id属性提取；.开头则依据class属性提取 具体CSS选择器的内容可以去http://www.w3school.com.cn/ urls = doc('#primary .post > a').items() items() 在pyquery中是一个生成器把所有的选中的许多生成一个列表，然后我们用for循环进行遍历每个标签并提取出链接 通过yield迭代器将链接返回到外层 返回html文档的函数不在赘述，很简单。接下来是解析html文档提取我们要的信息 def parse_html(html): doc = pq(html) title = doc('.title').text() time = doc('div.stuff > span:nth-child(1)').text() readcount = doc('div.stuff > span:nth-child(2)').text() content = doc('.content').text() return { 'title' : title, 'time' : time, 'readcount' : readcount, 'content' : content } 和提取herf一样 通过CSS 选择器获取我们索要的节点然后text() 即可提取所要的文本信息。之后以字典的形式返回数据到外层函数（因为数据库存储是以字典型） client = pymongo.MongoClient(MONGO_URl) db = client[MONGO_DB] def save_to_mongo(data): if db[MONGO_TABLE].update({'title' : data['title']},{'$set' : data}, True): print("Saved to Mongo", data['title']) else: print("Saved to Mongo Failed", data['title']) mongodb数据库的操作也极为简单，设定好MONGO_URL = ‘localhost’ MONGO_DB = ‘数据库名称’ MONGO_TABLE = ‘表名’我们采取更新的方式储存，如果信息已经出现我们选择更新，以title为关键字，如果title对应的数据已经出现那么就更新它更新的内容为{‘$set’ : data}，也就是data的全部数据，若最后一个参数为false 那么如果只执行更新操作，原来不存在的数据不会插入. MONGO 学习网站可以参考这个很简单http://www.runoob.com/mongodb/mongodb-update.html 其他储存照片也是一样的不在赘述贴一下代码 def save_picture(html): doc = pq(html) images = doc('#jg > a').items() if not os.path.exists(doc('title')): os.mkdir(doc('title')) for image in images: image = image.attr('href') try: response = requests.get(image) if response.status_code == 200: file_path = '{0}/{1}.jpg'.format(doc('.title').text(),md5(response.content).hexdigest()) if not os.path.exists(file_path): print("Downloading",file_path) with open(file_path,'wb') as f: f.write(response.content) else: print("Already Downloaded",file_path) except ConnectionError as e: print("Download Error", e.args) 需要注意的是储存文本时f.write()中的内容必须为字符串 一个简易爬虫就写好了，还差最后一个多进程问题 if __name__ == "__main__": pool = Pool() group = ([x for x in range(1,14)]) pool.map(main,group) pool.close() pool.join() pool = Pool() 没有指定的话会根据系统自动分配，通过map()函数,map(main,group)，第一个参数是方法，第二个是数组列表，简单的来说就是把main方法依次执行close() 关闭pool，使其不在接受新的任务。join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用具体请看大神的作品 https://cuiqingcai.com/3335 [链接]：https://pan.baidu.com/s/1lKcM4PgJaV-gKMuioQwaWQ 密码：gczk]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python链接数据库报错]]></title>
    <url>%2F2018%2F08%2F28%2Ffirst%2F</url>
    <content type="text"><![CDATA[在pycharm中连接数据库报错$ import hashlib,os,requests,ast $ import pymysql $ from bs4 import BeautifulSoup $ #数据库链接 $ conn=pymysql.connect(host="localhost",port=3306,user="root",passwd="1820401746yuan.A",db="pm") $ cursor=conn.cursor() #创建cursor对象 $ host 主机名也可用127.0.0.1.代替（本地服务器） port端口（附查看方式，登陆mysql输入下列代码） show global variables like 'port'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | port | 3306 | +---------------+-------+ 1 row in set, 1 warning (0.16 sec) user 用户名默认为root——权限最大的用户 ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO) 网上有很多的办法，但都不能解决我的问题。 比如赋予数据库权限，以及创建新用户等等。。。 然后在我的努力寻找下成功了 最简单的方法是更换了root密码的认证方式解决的，新版mysql使用的caching_sha2_password，换成mysql_native_password我就可以连上了。然后输入ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password'; password即你的密码） 原博客地址 https://blog.csdn.net/dongweionly/article/details/80273095]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
</search>
