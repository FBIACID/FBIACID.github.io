<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[希尔密码]]></title>
    <url>%2F2019%2F03%2F10%2F%E5%B8%8C%E5%B0%94%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[原理希尔密码是基于矩阵的线性变换,其最大的好处就是隐藏了字符的频率信息, 使得传统的通过字频来破译密文的方法失效。（例如维吉尼亚密码等都有统计规律） 加密者在对明文加密前会选择一个加密秘匙, 这个秘匙最终会以一个m矩阵的形式参与到加密算法中的. 在加密者选定了加密秘匙后, m便得到了确定,这时,加密者将明文按m个字母一组的形式分成多组, 最后一组不足m个字母的按特定的方式补齐. 这样就形成了很多组由m个字母组成的单个向量, 然后对每一个m阶向量, 我们用它去乘以确定好了的秘匙. 如下为其中的一个分组A向量加密后变为B向量的过程:[A1,A2,A3 … Am] * M = [B1,B2,B3 … Bm] .我们将所有相乘后的向量连在一起, 便得到了密文. 这便是希尔密码的加密. 密钥选取我们可以随便取矩阵作为密钥吗，当然不可能。需要两个先决条件 矩阵为非奇异矩阵（即det(A) != 0) gcd(det(A),26) = 1也就是说在模26的运算上密钥矩阵的值需要和26互质。选择非奇异矩阵是为了解密操作，没有逆矩阵我们就无法进行解密，那么这个加密算法就没有任何价值。 解密利用线性代数的知识我们可以清晰的知道解密只需要乘上密钥的逆矩阵即可，那么逆矩阵怎么求呢。 一是使用伴随矩阵, 通过计算行列式得到. 所用公式为: M^-1 = M^ / D . (其中M^为M的伴随矩阵, D为M的行列式的值) 伴随矩阵是和逆矩阵相似的一个概念，将我们的余子矩阵（准确的来说是代数余子式矩阵）转置就可以得到,然后将伴随矩阵乘以 1/（det(A)) 就可以得到逆矩阵。 ps: 这里说的是纯线性代数知识，1/(det(A)) 在数论中可不是简单的取倒数，而是求乘法逆元。 二是通过增广矩阵, 在M右侧附加一个n阶单位矩阵, 再通过初等变换将增广矩阵的左侧变换为一个n阶单位矩阵, 这时右侧便是我们需要的逆矩阵，这样会有一个缺点，比如这个矩阵： M = [[3,6],[2,7]] 它的逆矩阵为[[7/9,2/3].[-2/9,1/3]] 这个是不符合我们的要求的因为会损失精度，这可是绝对不可以出现的情况，这时就要采取其他的方法。 那么对于我们的例子直接乘9可以吗，显然这不可能。这里因为我们是Z26的字母表. 我们要保证乘以一个数之后, 原来的明文字母所增大的部分一定得是26的整数倍.（1）设x为扩大倍数 a为字符 t 为整数，则有 ax = a + 26t； t = a(x-1)/26 那么x = 26*p + 1 (p为整数）我们取p = 1即可，则x=27. 那么推广到矩阵就可以得知我们要乘的那个数字为（27，det(A))——27和矩阵值的最小公倍数。 （2）这个方法参见看雪论坛https://bbs.pediy.com/thread-89505.htm 具体代码实现就不写了，参见https://blog.csdn.net/kanamisama0/article/details/60836382]]></content>
      <categories>
        <category>密码学</category>
      </categories>
      <tags>
        <tag>密码学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转轮密码机]]></title>
    <url>%2F2019%2F03%2F09%2F%E8%BD%AC%E8%BD%AE%E5%AF%86%E7%A0%81%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[原理转轮密码机是多表代换密码的典型例子，在二战中广泛应用。分为三个轮子-快中慢，每次敲击一次字符轮子就会转动一次，从而打破了明文密文的固定代替关系。快轮子转动26次-即循环了字母表则中轮子会转动一个字符，整个密码机的周期为262626. 代码实现ElemType fgear[2][26] = { {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26},{8, 18, 26, 17, 20, 22, 10, 3, 13, 11, 4, 23, 5, 24, 9, 12, 25, 16, 19, 6, 15, 21, 2, 7, 1, 14} }; //快轮 ElemType mgear[2][26] = { {26, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25},{20, 1, 6, 4, 15, 3, 14, 12, 23, 5, 16, 2, 22, 19, 11, 18, 25, 24, 13, 7, 10, 8, 21, 9, 26, 17} }; //中轮 ElemType sgear[2][26] = { {24, 25, 26, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23},{21, 3, 15, 1, 19, 10, 14, 26, 20, 8, 16, 7, 22, 4, 11, 5, 17, 9, 12, 23, 18, 2, 25, 6, 24, 13} }; //慢轮 我们密码机的初态定义如上，应用循环链表来把初态输入我们的程序。 class Gear; //主类 class Node { private: ElemType leftElement; ElemType rightElement; Node *link; friend class Gear; }; //链表类 那么我们具体需要什么方法来实现呢，要有移动轮子的 move() 方法，还有把初态密码机输入程序的 InitGear() 方法，寻找每个轮子左列与输入下标相同的元素的 Search()方法， 当然右列也一样 Find() 方法，当然还有不可缺少的构造函数和析构函数（实际是完成Node链表的初始化和销毁） class Gear { public: Gear() { first = NULL; } //构造函数 ~Gear(); //析构函数 void initGear(ElemType data[2][26]); bool Find(ElemType in, ElemType &amp;x) const; //查找每个轮子左列与输入下标相同的元素的值 ElemType Search(ElemType x) const; //查找每个轮子右列与输入元素值相同的元素的下标 bool move(); //每加密一个字母，旋转一下轮子 private: Node *first; }; bool Gear::move() // { Node *p = first; for (ElemType i = 0; i &lt; 25; i++) { p = p->link; } first = p; return true; } Gear::~Gear() { Node *p=first->link,*q; //令p指向first的下一位，最后释放first指针 while (p!=first) { q = p->link; first->link = q; delete p; p = q; } delete first; } void Gear::initGear(ElemType data[2][26]) { Node *p=first, *q; p = first; for (ElemType i = 0; i &lt; 26; i++) { q = new Node; //创建新节点 q->leftElement = data[0][i]; q->rightElement = data[1][i]; if (i == 0) { first = q; p = q; } else if (i==25) //实现首尾相连 { p->link = q; p = q; q->link = first; } else { p->link = q; p = q; } } } ElemType Gear::Search(ElemType x) const { Node *p = first; ElemType i; for (i = 0; p &amp;&amp; p->rightElement != x; ++i) { //从第一个元素开始查找，一直到 p 指向 NULL 或 p 指向的节点的 element 值与 x 相等 p = p->link; } if (p) { //若 p 不指向 NULL，return 当前下标 return i; } return -1; } bool Gear::Find(ElemType in,ElemType &amp;x) const { Node *p; p = first; for (ElemType i = 0; i &lt; in; i++) { p = p->link; } x = p->leftElement; //利用x返回此时的值 return true; } ElemType Encode(ElemType in, Gear *gFast, Gear *gMiddle, Gear *gSlow) { ElemType x, result; gSlow->Find(in, x); result = gSlow->Search(x); gMiddle->Find(result, x); result = gMiddle->Search(x); gFast->Find(result, x); result = gFast->Search(x); return result; } //加密函数，模拟轮子的电路操作一一对应。 这里要停一下假如我们这样设置加密函数 ElemType Encode(ElemType in, Gear gFast, Gear gMiddle, Gear gSlow) 正常的传入我们的对象gFast,gMiddle,gSlow会发生什么 函数结束的时候对象会自动析构，要知道我们没有设置拷贝构造函数，但我们的对象中有动态空间（Node类，这里我们的Node附属于Gear的，会随着Gear消失而消失），就会发生指针指向NULL，从而GG程序宣告失败。这里用到的是C++的深浅拷贝问题以及函数的传参知识。 函数传参有三种传参方式：传值、传址、传引用。1、按值传递 （1）形参和实参各占一个独立的存储空间。 （2）形参的存储空间是函数被调用时才分配的，调用开始，系统为形参开辟一个临时的存储区，然后将各实参传递给形参，这是形参就得到了实参的值。2、地址传递 地址传递与值传递的不同在于，它把实参的存储地址传送给形参，使得形参指针和实参指针指向同一块地址。因此，被调用函数中对形参指针所指向的地址中内容的任何改变都会影响到实参。3、引用传递 引用传递是以引用为参数，则既可以使得对形参的任何操作都能改变相应数据，又使函数调用方便。引用传递是在形参调用前加入引用运算符“&amp;”。引用为实参的别名，和实参是同一个变量，则他们的值也相同，该引用改变则它的实参也改变。注：引用是C++的概念，C中没有，同时布尔类型也是C++的概念。 那么我们的转轮密码机做完了吗，显然还没有。我们只是搭好了架子做好了轮子还没有组装 void Encodeword(Gear gFast, Gear gMiddle, Gear gSlow) { char plain[100], cipher[100]; ElemType in[100], out[100]; ElemType i = 0; ElemType gf = 0, gm = 0; cout &lt;&lt; "请输入您要加密的内容（空格以'／'代替）: " &lt;&lt; endl; cin >> plain; ElemType n = (ElemType)(strlen(plain)); cout &lt;&lt; "正在加密......\n"; for (i = 0; i &lt; n; ++i) { if (plain[i] &lt; 65 || plain[i] > 122 || plain[i] == 91 || plain[i] == 92 || plain[i] == 93 || plain[i] == 94 || plain[i] == 95 || plain[i] == 96) { cipher[i] = plain[i]; } else { if (plain[i] >= 97 &amp;&amp; plain[i] &lt;= 122) { in[i] = plain[i] - 32 - 'A'; } else { in[i] = plain[i] - 'A'; } out[i] = Encode(in[i], &amp;gFast, &amp;gMiddle, &amp;gSlow); cipher[i] = char(out[i] + 'A'); gf++; gFast.move(); if (gf == 26) { gm++; gMiddle.move(); gf = 0; if (gm == 26) { gSlow.move(); gm = 0; } } } cipher[i + 1] = '\0'; } cout &lt;&lt; "加密成功！密文是：" &lt;&lt; endl; cout &lt;&lt; cipher; }*/ 有兴趣的可以自己写解密函数，还可以加上文件读写的功能使其可以对文件进行加密。]]></content>
      <categories>
        <category>密码学</category>
      </categories>
      <tags>
        <tag>密码学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代理的使用]]></title>
    <url>%2F2018%2F12%2F07%2F%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文记录一下代理的使用方法。 代理的分类代理有免费的和收费的。免费的一般很不稳定，可用率能有百分之10就不错了 国内有很多代理商，提供的代理质量也参差不齐，这里我们以阿步云为例，他提供两种隧道模式(协议的不同）。1——http隧道 专业版 动态版 经典版 2——socks隧道 经典版 http使用较多，我们就以它为例来说。专业版的代理会锁定一个ip 一分钟，到期后自动切换动态版顾名思义，每次请求都会更换一个新的ip经典版则是锁定一个ip 15分钟。 对于一个爬虫来说的话，更建议使用动态版，ip封禁后直接请求获取新的代理。阿步云给了详细的接入文档。 下面列出一个python版的 from urllib import request # 要访问的目标页面 targetUrl = &quot;http://test.abuyun.com/proxy.php&quot; #targetUrl = &quot;http://proxy.abuyun.com/switch-ip&quot; #targetUrl = &quot;http://proxy.abuyun.com/current-ip&quot; # 代理服务器 proxyHost = &quot;proxy.abuyun.com&quot; proxyPort = &quot;9010&quot; # 代理隧道验证信息 proxyUser = &quot;H01234567890123P&quot; proxyPass = &quot;0123456789012345&quot; proxyMeta = &quot;http://%(user)s:%(pass)s@%(host)s:%(port)s&quot; % { &quot;host&quot; : proxyHost, &quot;port&quot; : proxyPort, &quot;user&quot; : proxyUser, &quot;pass&quot; : proxyPass, } proxy_handler = request.ProxyHandler({ &quot;http&quot; : proxyMeta, &quot;https&quot; : proxyMeta, }) #auth = request.HTTPBasicAuthHandler() #opener = request.build_opener(proxy_handler, auth, request.HTTPHandler) opener = request.build_opener(proxy_handler) # opener.addheaders = [(&quot;Proxy-Switch-Ip&quot;, &quot;yes&quot;)] request.install_opener(opener) resp = request.urlopen(targetUrl).read() print (resp) 有四个参数需要我们填入 代理服务器 代理端口号 代理通行证书 代理通行密钥 代理服务器和端口号三个版本各自不同且唯一，通行证书和密钥则是在你购买之后才会得到直接复制代码然后填入对应参数就可以了。想要更多可以看这里 https://github.com/abuyun/proxy-demo/blob/master/python/python3/urllib/proxy-demo.py 假如我们要用selenium来操纵浏览器那么如何给他配置代理呢。这里我们只列举selenium + Chrome的方法 不用授权验证的代理 options = webdriver.ChromeOptions() options.add_argument(&#39;--proxy-server=http://ip:port&#39;) driver = webdriver.Chrome(executable_path=&quot;C:\chromedriver.exe&quot;, chrome_options=0ptions) driver.get(&quot;http://ip138.com/&quot;) print(driver.page_source) 调用ChromeOptions这个接口把我们的代理ip放进去 然后再把参数传入我们的打开浏览器的接口里，这里注意一下————代码里的executable_path=”C:\chromedriver.exe” 是我们chromedriver.exe所在的路径，如果我们已经把它放到了python的Scripts目录，那么可以省略这个参数。 需要授权验证的代理 import string import zipfile def create_proxyauth_extension(proxy_host, proxy_port, proxy_username, proxy_password, scheme=&#39;http&#39;, plugin_path=None): &quot;&quot;&quot;代理认证插件 args: proxy_host (str): 你的代理地址或者域名（str类型） proxy_port (int): 代理端口号（int类型） proxy_username (str):用户名（字符串） proxy_password (str): 密码 （字符串） kwargs: scheme (str): 代理方式 默认http plugin_path (str): 扩展的绝对路径 return str -&gt; plugin_path &quot;&quot;&quot; if plugin_path is None: plugin_path = &#39;vimm_chrome_proxyauth_plugin.zip&#39; manifest_json = &quot;&quot;&quot; { &quot;version&quot;: &quot;1.0.0&quot;, &quot;manifest_version&quot;: 2, &quot;name&quot;: &quot;Chrome Proxy&quot;, &quot;permissions&quot;: [ &quot;proxy&quot;, &quot;tabs&quot;, &quot;unlimitedStorage&quot;, &quot;storage&quot;, &quot;&lt;all_urls&gt;&quot;, &quot;webRequest&quot;, &quot;webRequestBlocking&quot; ], &quot;background&quot;: { &quot;scripts&quot;: [&quot;background.js&quot;] }, &quot;minimum_chrome_version&quot;:&quot;22.0.0&quot; } &quot;&quot;&quot; background_js = string.Template( &quot;&quot;&quot; var config = { mode: &quot;fixed_servers&quot;, rules: { singleProxy: { scheme: &quot;${scheme}&quot;, host: &quot;${host}&quot;, port: parseInt(${port}) }, bypassList: [&quot;foobar.com&quot;] } }; chrome.proxy.settings.set({value: config, scope: &quot;regular&quot;}, function() {}); function callbackFn(details) { return { authCredentials: { username: &quot;${username}&quot;, password: &quot;${password}&quot; } }; } chrome.webRequest.onAuthRequired.addListener( callbackFn, {urls: [&quot;&lt;all_urls&gt;&quot;]}, [&#39;blocking&#39;] ); &quot;&quot;&quot; ).substitute( host=proxy_host, port=proxy_port, username=proxy_username, password=proxy_password, scheme=scheme, ) with zipfile.ZipFile(plugin_path, &#39;w&#39;) as zp: zp.writestr(&quot;manifest.json&quot;, manifest_json) zp.writestr(&quot;background.js&quot;, background_js) return plugin_path 我们可以把这个写进单独的文件或者只作为一个函数,调用这个返回值 from selenium import webdriver from common.pubilc import create_proxyauth_extension proxyauth_plugin_path = create_proxyauth_extension( proxy_host=&quot;XXXXX.com&quot;, proxy_port=9020, proxy_username=&quot;XXXXXXX&quot;, proxy_password=&quot;XXXXXXX&quot; ) co = webdriver.ChromeOptions() # co.add_argument(&quot;--start-maximized&quot;) co.add_extension(proxyauth_plugin_path) driver = webdriver.Chrome(executable_path=&quot;C:\chromedriver.exe&quot;, chrome_options=co) driver.get(&quot;http://ip138.com/&quot;) print(driver.page_source) 我们可以不用了解上面的代码是什么原理，会用就行，当然理解最好。 还有一些常规浏览器的代理使用方法，阿布云都有详细的文档https://www.abuyun.com/http-proxy/dyn-manual.html 然后什么代理好用呢，已经有大神自己做过评测了，想知道请移步https://cuiqingcai.com/5094.html]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[set]]></title>
    <url>%2F2018%2F12%2F02%2Fset%2F</url>
    <content type="text"><![CDATA[本文记一下自己的程序设计周 往set容器内插入自定义类（自定义排序函数） 需要重载（） 或者&lt; 运算符，虽然它内部自定义了排序函数，但是对于我们自定义的类它无法自主判断 bool USER::operator&lt;(const USER &amp; demo) const { if (this-&gt;GetId_number() == demo.GetId_number()) //通过比较身份证号码去重 { return false; } else { if (Age != demo.Age) //如果年龄不同按年龄升序排列，如果相同则按身份证号码升序排列。 { return Age &lt; demo.Age; } else { return Id_number &lt; demo.Id_number; } } } 如果重载的是（）运算符那么是这样 bool USER::operator()(const USER &amp; demo,const USER &amp; demo2) const { if (demo-&gt;GetId_number() == demo2.GetId_number()) ////通过比较身份证号码去重 { return false; } else { if (demo.Age != demo2.Age) //如果年龄不同按年龄升序排列，如果相同则按身份证号码升序排列。 { return demo.Age &lt; demo2.Age; } else { return demo.Id_number &lt; demo2.Id_number; } } } 如何对容器内对象进行更改放入容器内的对象默认是不可更改的，是常对象。但一般我们还是需要对其更改，这时候我们只有采取先删除这个元素然后添加新元素的方法。 set&lt;USER&gt;::iterator p = user.begin(); 这里是我自己定义的类USER,user为已经保存数据的,采取生成迭代器方法，然后令指针指向容器的第一个元素。（注，begin()/end()不会检查容器是否为空，可用empty()检查）begin()和end()是指向第一个元素和最后一个元素吗？？有些博客上说end()是最后一个元素，其实是错的。end()指向最后一个元素之后为空的部分 删除元素-erase() ps-需要加一个判断，如果指针指向了容器的end()，没有办法删除操作。添加元素-insert() ps-不要忘记自定义排序函数 补一句既然是常对象，那么如果你要定义类函数访问他的某个属性，那么函数必须为常函数。 其他常见函数见https://zh.cppreference.com/w/cpp/container/set]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pyquery解析库]]></title>
    <url>%2F2018%2F11%2F26%2Fpq%2F</url>
    <content type="text"><![CDATA[本文简单的讲解一下Pyquery的简单操作。 Pyquery简介pyquery allows you to make jquery queries on xml documents. The API is as much as possible the similar to jquery. pyquery uses lxml for fast xml and html manipulation. This is not (or at least not yet) a library to produce or interact with javascript code. I just liked the jquery API and I missed it in python so I told myself “Hey let’s make jquery in python”. This is the result. It can be used for many purposes, one idea that I might try in the future is to use it for templating with pure http templates that you modify using pyquery. I can also be used for web scrapping or for theming applications with Deliverance. The project is being actively developped on a git repository on Github. I have the policy of giving push access to anyone who wants it and then to review what he does. So if you want to contribute just email me. Please report bugs on the github issue tracker. Jquery: a jquery-like library for python 安装Pyquerypip install pyquery 需要lxml,如果没有安装这个请自行安装 初始化Pyqueryfrom pyquery import PyQuery as pq #导入Pyquery起别名为pq 加载html字符串doc = pq(&quot;&lt;p style=&quot;text-align:center&quot;&gt;&lt;span style=&quot;font-size: 14px; line-height: 1.6; font-family: 微软雅黑, sans-serif;&quot;&gt;从前你以为军训休息时你可以尽情休息&lt;/span&gt;&lt;/p&gt;&quot;) 加载指定路径下的html文件doc = pq(&quot;C://**//**//demo.html&quot;) 我自己尝试报错了，懒得查看原因 加载指定url的html(即可以直接跳过requests请求，直接一步得到）doc = pq(url = &quot;http://www.baidu.com&quot;) 也可以改变请求头。 pq(&#39;http://cuiqingcai.com/&#39;, headers={&#39;user-agent&#39;: &#39; &#39;}) post请求也不在话下。 pq(&#39;http://httpbin.org/post&#39;, {&#39;foo&#39;: &#39;bar&#39;}, method=&#39;post&#39;, verify=True) 一般我们只用第一种，下面两种很少使用。 提取信息提取方法由很多种，熟悉jquery就会有无限可能，比如： You can use some of the pseudo classes that are available in jQuery but that are not standard in css such as :first :last :even :odd :eq :lt :gt :checked :selected :file: &gt;&gt;&gt; d(&#39;p:first&#39;) [&lt;p#hello.hello&gt;] 这里不多做演示 再比如我们可以采取jquery API的形式操作我们的属性。 &gt;&gt;&gt; p = pq(&#39;&lt;p id=&quot;hello&quot; class=&quot;hello&quot;&gt;&lt;/p&gt;&#39;)(&#39;p&#39;) &gt;&gt;&gt; p.attr(&quot;id&quot;) &#39;hello&#39; &gt;&gt;&gt; p.attr(&quot;id&quot;, &quot;plop&quot;) [&lt;p#plop.hello&gt;] &gt;&gt;&gt; p.attr(&quot;id&quot;, &quot;hello&quot;) [&lt;p#hello.hello&gt;] 如果你觉得这个不适应，我们还有稍微更Python一点的东西 &gt;&gt;&gt; p.attr.id = &quot;plop&quot; &gt;&gt;&gt; p.attr.id &#39;plop&#39; &gt;&gt;&gt; p.attr[&quot;id&quot;] = &quot;ola&quot; &gt;&gt;&gt; p.attr[&quot;id&quot;] &#39;ola&#39; &gt;&gt;&gt; p.attr(id=&#39;hello&#39;, class_=&#39;hello2&#39;) [&lt;p#hello.hello2&gt;] &gt;&gt;&gt; p.attr.class_ &#39;hello2&#39; &gt;&gt;&gt; p.attr.class_ = &#39;hello&#39; 我们经常会用到CSS选择器 使用CSS操纵class &gt;&gt;&gt; p.addClass(&quot;toto&quot;) [&lt;p#hello.hello.toto&gt;] &gt;&gt;&gt; p.toggleClass(&quot;titi toto&quot;) [&lt;p#hello.hello.titi&gt;] &gt;&gt;&gt; p.removeClass(&quot;titi&quot;) [&lt;p#hello.hello&gt;] -------------------------------- 使用CSS操纵style &gt;&gt;&gt; p.css(&quot;font-size&quot;, &quot;15px&quot;) [&lt;p#hello.hello&gt;] &gt;&gt;&gt; p.attr(&quot;style&quot;) &#39;font-size: 15px&#39; &gt;&gt;&gt; p.css({&quot;font-size&quot;: &quot;17px&quot;}) [&lt;p#hello.hello&gt;] &gt;&gt;&gt; p.attr(&quot;style&quot;) &#39;font-size: 17px&#39; -------------------------------- python一点的方法 &gt;&gt;&gt; p.css.font_size = &quot;16px&quot; &gt;&gt;&gt; p.attr.style &#39;font-size: 16px&#39; &gt;&gt;&gt; p.css[&#39;font-size&#39;] = &quot;15px&quot; &gt;&gt;&gt; p.attr.style &#39;font-size: 15px&#39; &gt;&gt;&gt; p.css(font_size=&quot;16px&quot;) [&lt;p#hello.hello&gt;] &gt;&gt;&gt; p.attr.style &#39;font-size: 16px&#39; &gt;&gt;&gt; p.css = {&quot;font-size&quot;: &quot;17px&quot;} &gt;&gt;&gt; p.attr.style &#39;font-size: 17px&#39; 我们还可以通过它来添加元素 末尾插入 &gt;&gt;&gt; d = pq(&#39;&lt;p class=&quot;hello&quot; id=&quot;hello&quot;&gt;you know Python rocks&lt;/p&gt;&#39;) &gt;&gt;&gt; d(&#39;p&#39;).append(&#39; check out &lt;a href=&quot;http://reddit.com/r/python&quot;&gt;&lt;span&gt;reddit&lt;/span&gt;&lt;/a&gt;&#39;) [&lt;p#hello.hello&gt;] &gt;&gt;&gt; print(d) &lt;p class=&quot;hello&quot; id=&quot;hello&quot;&gt;you know Python rocks check out &lt;a href=&quot;http://reddit.com/r/python&quot;&gt;&lt;span&gt;reddit&lt;/span&gt;&lt;/a&gt;&lt;/p&gt; 当然也可以在前面插入 &gt;&gt;&gt; p = d(&#39;p&#39;) &gt;&gt;&gt; p.prepend(&#39;check out &lt;a href=&quot;http://reddit.com/r/python&quot;&gt;reddit&lt;/a&gt;&#39;) [&lt;p#hello.hello&gt;] &gt;&gt;&gt; p.html() &#39;check out &lt;a href=&quot;http://reddit.com/r/python&quot;&gt;reddit&lt;/a&gt;you know ...&#39; 定位节点插入也不是不可能 &gt;&gt;&gt; d = pq(&#39;&lt;html&gt;&lt;body&gt;&lt;div id=&quot;test&quot;&gt;&lt;a href=&quot;http://python.org&quot;&gt;python&lt;/a&gt; !&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&#39;) &gt;&gt;&gt; p.prependTo(d(&#39;#test&#39;)) [&lt;p#hello.hello&gt;] &gt;&gt;&gt; d(&#39;#test&#39;).html() u&#39;&lt;p class=&quot;hello&quot; ...&#39; 删除元素也很简单 &gt;&gt;&gt; d = pq(&#39;&lt;html&gt;&lt;body&gt;&lt;p id=&quot;id&quot;&gt;Yeah!&lt;/p&gt;&lt;p&gt;python rocks !&lt;/p&gt;&lt;/div&gt;&lt;/html&gt;&#39;) &gt;&gt;&gt; d.remove(&#39;p#id&#39;) [&lt;html&gt;] &gt;&gt;&gt; d(&#39;p#id&#39;) [] &gt;&gt;&gt; d(&#39;p&#39;).empty() [&lt;p&gt;] 还有一些其他的操作这里不在细说，自己去参阅文档吧https://pythonhosted.org/pyquery/traversing.html 下面就该说说爬虫常用到的了 ##以CSS选择器为选择标志 doc = pq(html) content = doc(&quot;CSS选择器内容&quot;) 生成迭代器， content = doc(&quot;CSS选择器内容&quot;).items() 获取文本 content = doc(&quot;CSS选择器内容&quot;).text() 获取标签内的链接 content = doc(&quot;CSS选择器内容&quot;).attr(&#39;href&#39;) #如果这个选择器选出的对象唯一，否则需要遍历 其他不在多说。 补一句，前两天再写爬虫的时候，它会把嵌入的播放器页面的闭合标签（)自动去除，会有一些错误。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QT]]></title>
    <url>%2F2018%2F11%2F25%2FQT%2F</url>
    <content type="text"><![CDATA[本文简单的说一下，Qt一些坑。 文字兼容问题由于字符编码的储存不一样，Qt的中文显示会出现一些问题（如果仅仅是让它在代码里显示中文的话，打开时选择一下字符编码就好了） 解决办法如下（含一些已经失效的办法） 先将对应的cpp文件用windows自带的记事本打开，另存为UTF-8格式，然后在代码中，遇到中文字符，使用QStringLiteral(“中文”)进行修饰这种方式每次新建一个class就要切出去用记事本编辑一下实在麻烦，而且每个中文字符串都要用QStringLiteral来修饰，实在麻烦 在头文件申明中加上#pragma execution_character_set(&quot;utf-8&quot;) 一切OK了 但方法二已经废弃了，只可以用在QT5的最初几个版本。 此外还有一些办法 QString str1=QStringLiteral(&quot;中文字符&quot;); QString str2=QString::fromLocal8bit(&quot;中文字符&quot;); QString str3=QString::fromWCharArray(L&quot;中文字符&quot;); QString str4=u8&quot;中文字符&quot;; //好像这种方法只适合C++11以上版本 不过这些办法貌似还是没能解决我的问题——分别用QT和VS2017读取文本数据，中文就会变为一些奇妙的东西。（ps_暂时未找到合适的办法，以上是我网上找了好多博客总结的办法，当然最简单的就是你其中一个打开就好了） 多界面切换问题主界面（Widget) 对话框（Dialog)也可以称为子界面 对于一个界面我们有这么几种操作可以做 close() 关闭界面，如果是主界面那么直接退出程序 hide() 隐藏界面，一般用于界面转换的时候，由主界面转向子界面 show() 显示一个界面以及一些其他的更改界面大小之类的操作 当我们从主界面跳转至子界面主界面中我们这么写ps– this-&gt;hide();new-&gt;exec();this-&gt;show();子界面可以 槽函数clicked() 和 accept(); close(); 这里说一下show() 和 exec()的区别 show()非模式对话框显示一个非模式对话框。控制权即刻返回给调用函数。弹出窗口是否模式对话框，取决于modal属性的值。（原文：Shows the dialog as a modeless dialog. Control returns immediately to the calling code. The dialog will be modal or modeless according to the value of the modal property. ） exec()模式对话框显示一个模式对话框，并且锁住程序直到用户关闭该对话框为止。函数返回一个DialogCode结果。在对话框弹出期间，用户不可以切换同程序下的其它窗口，直到该对话框被关闭。（原文：Shows the dialog as a modal , blocking until the user closes it. The function returns a DialogCode result.Users cannot interact with any other window in the same application until they close the dialog. ） 也就是说如果是exec()那么只有这个操作结束返回之后才会进行下一步操作，也就是阻塞（开启一个事件循环，所谓事件循环，可以理解成一个无限循环。Qt 在开启了事件循环之后，系统发出的各种事件才能够被程序监听到。这个事件循环相当于一种轮询的作用。既然是无限循环，当然在开启了事件循环的地方，代码就会被阻塞，后面的语句也就不会被执行到。因此，对于使用了exec()显示的模态对话框，我们可以在exec()函数之后直接从对话框的对象获取到数据值。）通俗的说就是运用exec()之后我们只有对这个对话框完成操作后才能进行其他的，而show()则可以同时进行其他操作（默认情况下）。exec()是有返回值的，QDialog::Accepted or QDialog::Rejected当然我们调用槽函数accept() 也可以在show()中达到这个操作。 显示数据可直接更改问题。ui-&gt;tableView-&gt;setEditTriggers(QTableView::NoEditTriggers); 设置不可编辑成功这是只针对TableView的。其他的也类似。 先说说如何显示数据吧，有三种 Table View (即以表格的形式显示` 需要引用QStandardItemModel头文件 QStandardItemModel *model = new QStandardItemModel(); model-&gt;setColumnCount(7); //设置列数 model-&gt;setHeaderData(0,Qt::Horizontal,QString::fromStdString(“XX”)); //设置每列的名称 model-&gt;setHeaderData(1,Qt::Horizontal,QString::fromStdString(“XX”)); model-&gt;setHeaderData(2,Qt::Horizontal,QString::fromStdString(“XXX”)); model-&gt;setHeaderData(3,Qt::Horizontal,QString::fromStdString(“XX”)); model-&gt;setHeaderData(4,Qt::Horizontal,QString::fromStdString(“XX”)); model-&gt;setHeaderData(5,Qt::Horizontal,QString::fromStdString(“XX”)); model-&gt;setHeaderData(6,Qt::Horizontal,QString::fromStdString(“XXX”)); ui-&gt;tableView-&gt;setModel(model); //在界面中显示我们设置的表格 当然也可以逐个读取 QStringList headers; headers &lt;&lt; “ID” &lt;&lt; “Name” &lt;&lt; “Age” &lt;&lt; “Sex”; tableWidget.setHorizontalHeaderLabels(headers); 读入数据 ui-&gt;tableView-&gt;setEditTriggers(QTableView::NoEditTriggers); for(int i = 0;i &lt; 7;i++) { ui-&gt;tableView-&gt;setColumnWidth(i,70); 设置宽度 } set::iterator p = Index::user.begin(); model-&gt;removeRows(0,model-&gt;rowCount()); //删除行，也就是初始化显示界面 for(int i = 0;i &lt; Index::user.size();i++) { model-&gt;setItem(i,0,new QStandardItem(QString::fromStdString(p-&gt;GetName()))); model-&gt;setItem(i,1,new QStandardItem(QString::fromStdString(p-&gt;GetSex()))); model-&gt;setItem(i,2,new QStandardItem(QString::fromStdString(p-&gt;GetId_number()))); model-&gt;setItem(i,3,new QStandardItem(QString::number(p-&gt;GetAge()))); model-&gt;setItem(i,4,new QStandardItem(QString::fromStdString(p-&gt;GetAddress()))); model-&gt;setItem(i,5,new QStandardItem(QString::fromStdString(p-&gt;GetCareer()))); model-&gt;setItem(i,6,new QStandardItem(QString::fromStdString(p-&gt;GetPhone_number()))); p++; } 具体看你自己的程序 List View (即以列表的形式） QStringList user;user += “first”;user +=”second”;QStringListModel *model = new QStringListModel(user);userList-&gt;setModel(model); //userList是个QListViewuser += “third”;model-&gt;setStringList(user);model-&gt;removeRows(0,model-&gt;rowCount());//从第0行开始，删除所有行model-&gt;removeRows(0,2);//从第0行开始，删除前2行 Tree View (即以树状图形式）这个比较详细 我不在多说http://www.it165.net/pro/html/201405/14029.html ` 以后再做补充]]></content>
      <categories>
        <category>QT</category>
      </categories>
      <tags>
        <tag>QT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度下降算法]]></title>
    <url>%2F2018%2F10%2F31%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文简单介绍一下Logistic回归中寻找最优化系数的梯度下降算法。 何为回归拟合假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。利用Logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类。这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数集。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法。 我们以二分类为例，也就是将你的数据集分为0-1两类，我们要尽可能的确保他们的分类正确率，使我们的错误率下降到最低。我们需要找到一个合适的函数，对我们的输入数据可以分成两类。 Sigmoid函数先来介绍一个函数，在两个类的情况下，函数输出0或1。或许你之前接触过具有这种性质的函数，该函数称为海维塞德阶跃函数（Heaviside step function），或者直接称为单位阶跃函数。但这个函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程有时很难处理。 幸好另一个函数也有相似的性质，没错就是Sigmoid函数:y = 1/(1+e-z)函数图像如图所示： 可见在区间足够大的时候，很像阶跃函数，数学上也很好处理。当x=0时，函数值为0.5，当x&gt;0,函数值直线上升接近1；x&lt;0时则急速下降至0附近。 回到我们的分类器上来，既然找到了这么一个可以解决我们问题的函数，那么我们就为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和代入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5即被归入0类。现在这个分类器的关键在于如何找到一个最佳的回归系数，也到了引入我们的梯度下降算法的时候的。 梯度下降算法运用梯度下降算法我们需要输入我们所有的特性值。形式如下： h(x) = w0x0 + w1x1 + …….+ wn*xn [w0,w1,…….wn] 为我们要的回归系数向量而x0…….xn 为我们的特征值向量。我们怎么应用它呢，这时要引入另一个函数了：代价函数。 代价函数 代价函数也就是我们的特征值经过h(x)进行运算之后得到的值与真值进行对比，我们采取平方和来定义这个函数，m个数据集的预测值与真实值差的平方和，除以数据集数目的2倍，这就是我们代价函数的定义。代价函数的下降是用初始的向量值减去（学习率乘代价函数的偏导数），从而得到我们的新的代价函数，一般是逐渐下降的接近与0的时候我们的最佳回归系数也就确定了。 随着代价函数的下降，其实和我们梯度下降一个道理，以同样的形式对回归系数进行更新。 代价函数是我们判断这个回归系数怎么样的一个标准，代价函数越小那么我们的回归系数就越适合，也就意味着我们的分类器的正确率也就越高，因此这是一个非常重要的函数。 如何界定最佳系数算法已经完成，那么我们怎么确定一个值让他停下呢。我们有几种思路： 通过设定一个阈值，当我们的代价函数小于这个值的时候，我们就中止算法，选择这时的回归系数。 设定循环次数，界定一个循环次数，因为阈值很难界定，我们不知道这个阈值需要进行多久的运算，也许我们某个学习率下代价函数不可能小于阈值，那么我们的实验就失败了，因此设定一个循环次数是一般采用的方法。 数据归一化我们的数据不总是合适的，一个数据集的特征值千差万别，比如我们的房屋面积和房屋年龄，两个数据大概是几十倍，相差这么大的数据会让我们的算法出现很大的难度，所以我们需要进行数据归一化。简单的介绍两种 特征值缩放-我们取出每种特征值的最大值和最小值，然后让每个特征值减去最小值，再除去最大值和最小值的差，然后就可以得到0-1的特征值向量 均值归一化，取出每种特征值的最大最小值和平均值，让每个特征值减去平均值，然后除去最大值最小值的差，即完成均值归一化 如何选择合适的学习率呢一个合适的学习率，能够让我们很快的得到最佳回归系数；一个很烂的学习率，也许会让我们的代价函数越来越大，越来越偏离我们的最佳回归系数，如图所示： 学习率太小的话，我们的代价函数需要经过很久很久的循环才能下降到合适值；学习率过大则会使我们的代价函数出现很大的波动，因此学习率真的很重要，只有通过多次实验观察才能找到合适的值，一般我们采用0.001，0.01,0.1为学习率，适当修改就可以满足需要。 正规方程最佳回归系数只有通过梯度下降（上升）吗，当然不是我们还有一个正规方程可以用，形式如下：这么一个方程就可以完成最佳回归系数的求解，当然它也是有局限性的，因为如果我们的特征值很多，数据集很大，上万条数据，那么我们进行矩阵的运算，就会对电脑造成很大的负担，很难得到最终结果，计算复杂度太大。这里展示一下梯度下降(上升）法，和正规方程的利与弊： 具体选择还需多多实践，通常如果数据集乘特征值的量不超过万，那么正规方程是个良好的选择，不过一般真正的数据下，还是梯度算法更有意义。 本文不在贴代码，简单的介绍，还需多加实践。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三感故事]]></title>
    <url>%2F2018%2F10%2F23%2Fgudu%2F</url>
    <content type="text"><![CDATA[刷朋友圈的时候，看到一条由三感音乐故事拍摄的短视频，被文案戳中了泪点。 0122岁生日，一个人吃火锅，还好锅底可以点最辣的；187次路过的码头，4次遇到一对情侣，两个人眼中的风景，也不见得更好看；第6次 一个人搬家，扔掉了3箱旧东西，很遗憾，好像连回忆也一并被丢弃了。8点场的电影，48块的双人套餐，两杯可乐一个人喝，喝到的心情都有点丧丧的；第12次一个人输液，200ml的药水，一共2386滴，37分钟滴完，但痛了很久；一个人过生日，8寸的蛋糕，颜色有5种。 看到这些，我不由得联想到自己的状态。 住学校周转房的11楼，因为没有通电梯，同事们都没搬进去，那一层只有我一个人。尽管房子花了四五万装修，买了沙发茶几，装了电视，贴了墙纸，但还是没有家的感觉。 有电冰箱、零食柜，但里面都是空的；有微波炉、电磁炉，但从来没用过。 家里一共6盏灯，现在已经坏了4盏，我没想着去修一下。于我而言，黑暗冷清和灯火通明，并没有什么区别。 每周五下午开始，就没人再和我说话，等到周日晚上返校开班会的时候，前五分钟的讲话连吐字都不清楚，需要重新练习发声。 我也渴望被理解，但我讨厌去解释。 02一个人生活，细细数来已经快10年了。 背井离乡到外地求学，毕业后在冰冷的钢筋森林里打拼。学着在一个陌生的环境中，处理人际关系，打理生活琐事，去应对生活里的各种暴击。 一个人吃饭，点两菜一汤，老板不愿浪费，自作主张少炒了一道菜。一个人吃火锅，服务员反复强调要按两人份收钱，不然他们就亏了。 一个人去KTV，开了间包厢，唱了好多遍《好久不见》和《盛夏的果实》，想唱《屋顶》没人陪，叫来包房公主，但她不喜欢周杰伦。 一个人去医院，住的地方叫不到车，已经吐到连胃酸都没得吐了，还要强撑着开车，难受的时候靠边停一会儿，即便头晕目眩也要保持清醒底线，得分清刹车和油门。 一个人吃烧烤，豆角、花菜、豆腐串、烤肉、韭菜、茄子、鱼片，每次都必点，老板已经很熟悉了，去的时候会安排和别人拼桌，然后直接把单子报一遍。 一个人旅行，下榻在桂林，那夜雷雨，全区停电。起初因为雷电太响，就躲得离窗户远一些。后来干脆拉开窗帘，坐在窗台上，眼前一片漆黑，只有闪电在天空肆意纵横，雨势滂沱，雷声滚滚。 我想着那些黑洞洞的窗口里，人们都蜷缩躲着，就忽而觉得这是一场盛宴，美得惊心动魄，不药可救。 人生中大部分时候，我都无比厌恶自己，那一刻我觉得自己很棒。 032018年的第一天，我看到一个大四的小姑娘在朋友圈许愿：“我的新年愿望，是希望能买一台全自动洗衣机，实在不想碰冰水了。” 那是我用刀片划拉手腕重生的第二天，我决定帮她实现愿望。 我让她下单后找我代付，很快就看见她更新朋友圈：“新年愿望达成，再也不用碰冰水了。” 前些天，我又看到她一个人去一个偏远的地方上班，打不到车，叫不了外卖。住的地方什么也没有，生活用品和铺盖都得自己张罗，做到年底可以拿4千块。 她找我诉苦：“太冷了，太孤独了，太荒凉了，原本是想毕业了不好意思再让父母养着就出来试试，说不定以后还可以考到这里工作，没想到这么艰难，突然就迷茫了。” 我说：“不要迷茫，要想清楚，如果只为赚那点工资，就早点撤离，生活和感情一样，都需要及时止损。一天都忍受不了的人，别想着可以将就一辈子。一天都呆不下去的地方，别想着可以凑合一辈子。” 她说：“可是我不想回家，我妈知道我辞职会杀了我。” 我说：“那你回学校准备考工作，就考你想去的地方，经过这样一番折腾，你应该就有动力复习了。” 她说：“可是我没有车费。” 于是，我又给她转了4千块。 转完钱我就把她拉黑了，因为我并不关心她以后过得怎么样。我肯帮她，只是庆幸自己好不容易熬过那个冰冷的夜晚，想在这样一个寒冷的冬天，给陌生人施与一些暖意。 她辞职后，应该会一个人把买好的东西打包好，再搬回学校，应该会一个人做饭、看书、刷题，工作考了一次又一次，然后被无情刷掉。 可是，这些都没什么大不了，生命中有一段严重孤独的时光是好事。它得隔绝外界的打扰和帮助，是一个人从培养社会性转向自我探索的非常好的契机。 经过这个过程，你可以完成对自己的重新评估，学会信任自己，习惯独自行动，看轻外部评价，实现人生的种种可能。 04电影《冈仁波齐》中，有一个镜头让我很感动。一个朝圣者趴在地上，看着一只蚂蚁缓缓在他面前爬过。 我感动的点，不是他给一只蚂蚁让路，毕竟这种行为沾染了都市文明中马路文化的傲慢。我感动的，是他“意识”到蚂蚁的存在，那是一只蚂蚁，不是一个1厘米不到的黑点图形。 在他意识到蚂蚁的那一刻，他的孤独是有质感的，因为他感受到了大自然中其他生命的繁忙。 那朱雀桥边，乌衣巷口， 金陵玉树，秦淮水榭，多少热闹是非成败，转头都变成了英雄冢。可是海滩的岩石，巍峨的高山，奔腾的激流 ，却从来都是那样，他们孤独而不朽。 孤独真的很糟糕吗？我很喜欢短片里的回答。 其实，什么样的状态，都总是有寂寞、失落、安静、绝望的日子。 你离群索居，独自前行，是因为你知道所谓孤独，不过是害怕孤独，于是你耐住了寂寞，挨过了失落，学会了安静，扛住了绝望。 寂寞是一种情绪，孤独是一种境境界。人终要是走出咖啡屋，尘世的喜怒哀乐的冲击往往更加强烈，更让人难以忘怀。 只有耐得住寂寞，忍得了孤独，你才能让心静下来，学到更多的东西，获得独一份的成长。 世界上没有两片相同的树叶，没有什么比自己陪伴自己，更让人幸福。 05生而为人，我们太喜欢形式上的丰盈，并在无人问津的时刻变得脆弱，这是对自己不负责任的表现。 一个人来时，不管他身后多么丰盈，早晚也要独自面对这个世界，完全依靠自己内心的力量存活下去。 千万不要一遇到困难，就慌不择路到处找人求助，不敢自由思考和独立行动。千万不要刚探出一点头，又被几句外界评价打消，迅速缩回安全但逼仄的壳里。 正如短片里说的那样，所谓孤独，不过是一场无人送粥的重感冒。疼总是外界给的，但病总是自己好的。我要放肆吃喝，放肆笑闹，我要跟我的灵魂说话，我孤独但我不寂寞。 要么孤独，要么庸俗。扛得住孤独，世界才能被你左右。 作者：衷曲无闻BGM：再见我的爱人來源：简书]]></content>
      <categories>
        <category>情感</category>
      </categories>
      <tags>
        <tag>情感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-网页基础]]></title>
    <url>%2F2018%2F10%2F21%2Fclass0-1%2F</url>
    <content type="text"><![CDATA[本文讲一下网页的基本组成、结构、节点等内容。 网页的构成网页由三大部分，HTML,CSS,JavaScript,我们把网页比作一个人的话，HTML 相当于骨架，JavaScript 则相当于肌肉，CSS 则相当于皮肤，三者结合起来才能形成一个完善的网页 HTMLHTML 是用来描述网页的一种语言，其全称叫做 Hyper Text Markup Language，即超文本标记语言。网页包括文字、按钮、图片、视频等各种复杂的元素，其基础架构就是 HTML。不同类型的文字通过不同类型的标签来表示，如图片用 img 标签表示，视频用 video 标签来表示，段落用 p 标签来表示，它们之间的布局又常通过布局标签 div 嵌套组合而成，各种标签通过不同的排列和嵌套才形成了网页的框架。 CSSHTML 定义了网页的结构，但是只有 HTML 页面的布局会不美观，可能只是简单的节点元素的排列，那么为了让网页看起来更好看一点，在这里就借助于 CSS。CSS，全称叫做 Cascading Style Sheets，即层叠样式表。“层叠”是指当在 HTML 中引用了数个样式文件，并且样式发生冲突时，浏览器能依据层叠顺序处理。“样式”指网页中文字大小、颜色、元素间距、排列等格式。CSS是目前唯一的网页页面排版样式标准，有了它的帮助，页面才会变得更为美观。例如： #head_wrapper.s-ps-islite .s-p-top { position: absolute; bottom: 40px; width: 100%; height: 181px; } 这就是一个 CSS 样式，在大括号前面是一个 CSS 选择器，此选择器的意思是选中 id 为 head_wrapper 且 class 为 s-ps-islite 内部的 class 为 s-p-top 的元素。大括号内部写的就是一条条样式规则，例如 position 指定了这个元素的布局方式为绝对布局，bottom 指定元素的下边距为 40 像素，width 指定了宽度为 100% 占满父元素，height 则指定了元素的高度。也就是说我们将一些位置、宽度、高度等样式配置统一写成这样的形式，大括号括起来，然后开头再加上一个 CSS 选择器，就代表这一个样式对 CSS 选择器选中的元素生效，这样元素就会根据此样式来展示了。所以在网页中，一般会统一定义整个网页的样式规则，写入到 CSS 文件，其后缀名为 css，在 HTML 中只需要用 link 标签即可引入写好的 CSS 文件，这样整个页面就会变得美观优雅。 JavaScriptJavaScript，简称为 JS，是一种脚本语言，HTML 和 CSS 配合使用，提供给用户的只是一种静态的信息，缺少交互性。我们在网页里可能会看到一些交互和动画效果，如下载进度条、提示框、轮播图等，这通常就是 JavaScript 的功劳。它的出现使得用户与信息之间不只是一种浏览与显示的关系，而是实现了一种实时、动态、交互的页面功能。JavaScript 通常也是以单独的文件形式加载的，后缀名为 js，在 HTML 中通过 script 标签即可引入 网页结构&lt;!DOCTYPE html> &lt;html> &lt;head> &lt;meta charset="UTF-8"> &lt;title>This is a Demo&lt;/title> &lt;/head> &lt;body> &lt;div id="container"> &lt;div class="wrapper"> &lt;h2 class="title">Hello World&lt;/h2> &lt;p class="text">Hello, this is a paragraph.&lt;/p> &lt;/div> &lt;/div> &lt;/body> &lt;/html> 这就是一个最简单的 HTML 实例，开头是 DOCTYPE 定义了文档类型，其次最外层是 html 标签，最后还有对应的结尾代表标签闭合，其内部是 head 标签和 body 标签，分别代表网页头和网页体，它们也分别需要尾标签表示闭合。head 标签内定义了一些页面的配置和引用，如： &lt;meta charset="UTF-8"> 定义了字符编码为UTF-8。其他不在赘述，很简单。 节点及节点关系在 HTML 中，所有标签定义的内容都是节点，它们构成了一个 HTML DOM 树。（我们在开发者工具中见到的都是DOM树结构的代码）DOM，英文全称 Document Object Model，即文档对象模型。它定义了访问 HTML 和 XML 文档的标准W3C DOM 标准被分为 3 个不同的部分：1-核心 DOM - 针对任何结构化文档的标准模型2-XML DOM - 针对 XML 文档的标准模型3-HTML DOM - 针对 HTML 文档的标准模型根据 W3C 的 HTML DOM 标准，HTML 文档中的所有内容都是节点：1-整个文档是一个文档节点2-每个 HTML 元素是元素节点3-HTML 元素内的文本是文本节点4-每个 HTML 属性是属性节点5-注释是注释节点HTML DOM 将 HTML 文档视作树结构，这种结构被称为节点树通过 HTML DOM，树中的所有节点均可通过 JavaScript 进行访问，所有 HTML 节点元素均可被修改，也可以被创建或删除。节点树中的节点彼此拥有层级关系。我们常用 parent（父）、child（子）和 sibling（兄弟）等术语用于描述这些关系。父节点拥有子节点，同级的子节点被称为兄弟节点。在节点树中，顶端节点被称为根（root），除了根节点之外每个节点都有父节点，同时可拥有任意数量的子节点或兄弟节点。(数据库的最高权限用户为Root一样的道理，是根的意思） 选择器在 CSS 中是使用了 CSS 选择器来定位节点的，例如上例中有个 div 节点的 id 为 container，那么我们就可以用 CSS 选择器表示为 #container，# 开头代表选择 id，其后紧跟 id 的名称。另外如果我们想选择 class 为 wrapper 的节点，便可以使用 .wrapper，. 开头代表选择 class，其后紧跟 class 的名称。另外还有一种选择方式是根据标签名筛选，例如我们想选择二级标题，直接用 h2 即可选择。如上是最常用的三种选择表示，分别是根据 id、class、标签名筛选，请牢记它们的写法。另外 CSS 选择器还支持嵌套选择，各个选择器之间加上空格分隔开便可以代表嵌套关系，如 #container .wrapper p 则代表选择 id 为 container 内部的 class 为 wrapper 内部的 p 节点。另外如果不加空格则代表并列关系，如 div#container .wrapper p.text 代表选择 id 为 container 的 div 节点内部的 class 为 wrapper 节点内部的 class 为 text 的 p 节点。这就是 CSS 选择器，其筛选功能还是非常强大的：具体请自行查阅官方文档。 Session 和 cookies不知有没有人考虑过，服务器是怎么知道我们是否登录的，为什么有时候长时间未操作的已登陆界面会自己退出登陆。这些都要归功于他们。上面我们贴了一段最简单的Html代码，我们将其保存为一个 html 文件，然后把它放在某台具有固定公网 IP 的主机上，主机上装上 Apache 或 Nginx 等服务器，这样这台主机就可以作为服务器了，其他人便可以通过访问服务器看到这个页面了，这就搭建了一个最简单的网站。这种网页的内容是 HTML 代码编写的，文字、图片等内容均是通过写好的 HTML 代码来指定的，这种页面叫做静态网页。这种网页加载速度快，编写简单，但是存在很大的缺陷，如可维护性差，不能根据 URL 灵活多变地显示内容等,和后台没有交互，不能满足我们的需求，只能看不能玩的花架子。所以动态网页应运而生，它可以动态解析 URL 中参数的变化，关联数据库并动态地呈现不同的页面内容，非常灵活多变，我们现在遇到的大多数网站都是动态网站，它们不再是一个简单的 HTML，而是可能由 JSP、PHP、Python 等语言编写的，功能相比静态网页强大和丰富太多太多。动态网站还可以实现用户登录注册的功能，再回到开篇提到的问题，很多页面是需要登录之后才可以查看的，按照一般的逻辑来说，我们输入用户名密码登录之后，肯定是拿到了一种类似凭证的东西，有了它我们才能保持登录状态，才能访问登录之后才能看到的页面 无状态Http在了解 Session 和 Cookies 之前，我们还需要了解 HTTP 的一个特点，叫做无状态. HTTP 的无状态是指 HTTP 协议对事务处理是没有记忆能力的，也就是说服务器不知道客户端是什么状态。当我们向服务器发送一个 Requset 后，服务器解析此 Request，然后返回对应的 Response，服务器负责完成这个过程，而且这个过程是完全独立的，服务器不会记录前后状态的变化，也就是说它左耳朵进右耳朵出，并没有把我们的事件放在心里。特别是需要登陆的系统，想象一下，每次操作都要登陆，多么刺激！！！！ 这时候，两个用于保持 HTTP 连接状态的技术就出现了，它们分Session 和 Cookies，Session 在服务端，也就是网站的服务器，用来保存用户的会话信息，Cookies 在客户端，也可以理解为浏览器端，有了 Cookies，浏览器在下次访问网页时会自动附带上它发送给服务器，服务器通过识别 Cookies 并鉴定出是哪个用户，然后再判断用户是否是登录状态，然后返回对应的 Response。它们唯一对应，理解为Cookies是一个包含你信息的档案袋，有了它学校就知道你大概做过了什么，是不是本校的学生。（栗子不恰当请见谅） SessionSession，即会话，其本来的含义是指有始有终的一系列动作/消息，比如你大学本科从报名到拿到毕业证的过程。而在 Web 中 Session 对象用来存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当用户请求来自应用程序的 Web 页时，如果该用户还没有会话，则 Web 服务器将自动创建一个 Session 对象。当会话过期或被放弃后，服务器将终止该会话。（比如你的本科学生证享受的优惠只持续四年(杠精说编级的我也没办法))。 CookiesCookie，有时也用其复数形式 Cookies，指某些网站为了辨别用户身份、进行 Session 跟踪而储存在用户本地终端上的数据。 主要用来和服务器验证我们做了什么。当客户端第一次请求服务器时，服务器会返回一个 Headers 中带有 Set-Cookie 字段的 Response 给客户端，用来标记是哪一个用户，客户端浏览器会把Cookies 保存起来。当浏览器下一次再请求该网站时，浏览器会把此Cookies 放到 Request Headers 一起提交给服务器，Cookies 携带了 Session ID 信息，服务器检查该 Cookies 即可找到对应的 Session 是什么，然后再判断 Session 来以此来辨认用户状态。 登陆后服务器的Session会和你的cookies匹配，如果某些变量有效系统就是认为你是登陆的用户，给你返回相应的资源，但这是有时间限制的，如果传给服务器的 Cookies 是无效的，或者 Session 已经过期了，我们将不能继续访问页面，可能会收到错误的 Response 或者跳转到登录页面重新登录。所以 Cookies 和 Session 需要配合，一个处于客户端，一个处于服务端，二者共同协作，就实现了登录会话控制。 cookies结构在浏览器开发者工具中打开 Application 选项卡，然后在左侧会有一个 Storage 部分，最后一项即为 Cookies，将其点开，可以看到类似如下内容，这些就是 Cookies，如图所示：我们可以看到 Cookies 有一个个条目，每个条目我们可以称之为 Cookie，取单数形式。它有这么几个属性： Name，即该 Cookie 的名称。Cookie 一旦创建，名称便不可更改Value，即该 Cookie 的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。 Max Age，即该 Cookie 失效的时间，单位秒，也常和 Expires 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该Cookie 在 Max Age 秒之后失效。如果为负数，则关闭浏览器时Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。 Path，即该 Cookie 的使用路径。如果设置为 /path/，则只有路径为 /path/ 的页面可以访问该 Cookie。如果设置为/，则本域名下的所有页面都可以访问该 Cookie。 Domain，即可以访问该 Cookie 的域名。例如如果设置为 .zhihu.com，则所有以 zhihu.com，结尾的域名都可以访问该Cookie。 Size字段，即此 Cookie 的大小。 Http字段，即 Cookie 的 httponly 属性。若此属性为 true，则只有在 HTTP Headers 中会带有此 Cookie 的信息，而不能通过 document.cookie 来访问此 Cookie。Secure，即该 Cookie 是否仅被使用安全协议传输。安全协议。安全协议有 HTTPS，SSL 等，在网络上传输数据之前先将数据加密。默认为 false。以上便是 Cookies 的基本结构。 会话cookies 和持久cookies表面意思来说，会话 Cookie 就是把 Cookie 放在浏览器内存里，浏览器在关闭之后该 Cookie 即失效，持久 Cookie 则会保存到客户端的硬盘中，下次还可以继续使用，用于长久保持用户登录状态 严格来说不可以这样定义，它只是由 Cookie 的 Max Age 或 Expires 字段决定了过期的时间，通过它浏览器可以计算出其有效时间。Max Age 如果为正数，则该 Cookie 在 Max Age 秒之后失效，如果 Max Age 特别大，那就会保存非常长的时间。如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会以任何形式保存该 Cookie。 常见误区 session 随着浏览器关闭就消失了。ps:你办的会员卡会被商家主动注销吗？显然不会。 当我们关闭浏览器时，浏览器不会主动在关闭之前通知服务器它将要关闭，所以服务器根本不会有机会知道浏览器已经关闭，之所以会有这种错觉，是大部分 Session 机制都使用会话 Cookie 来保存 Session ID 信息，而关闭浏览器后 Cookies 就消失了，再次连接服务器时也就无法找到原来的 Session。如果服务器设置的 Cookies 被保存到硬盘上，或者使用某种手段改写浏览器发出的 HTTP 请求头，把原来的 Cookies 发送给服务器，则再次打开浏览器仍然能够找到原来的 Session ID，依旧还是可以保持登录状态的。ps:暗示爬虫 恰恰是由于关闭浏览器不会导致 Session 被删除，这就需要服务器为 Seesion 设置一个失效时间，当距离客户端上一次使用 Session 的时间超过这个失效时间时，服务器就可以认为客户端已经停止了活动，才会把 Session 删除以节省存储空间。 参考资料：- http://www.mamicode.com/info-detail-46545.html https://en.wikipedia.org/wiki/HTTP_cookie]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>web网页</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-HTTP基础]]></title>
    <url>%2F2018%2F10%2F21%2Fclass0%2F</url>
    <content type="text"><![CDATA[本文主要讲一下写爬虫必须大体了解的网页基础-HTTP 简单的说一下写一个爬虫，我们不能盲人摸象，什么都不知道就随便的请求，即使我们熟悉那些基本库，但不了解网页还是无法完成一个真正的爬虫。主要讲解一下基本知识。 HTTP基本原理URL 和 URI我们经常会听到 URI 和 URL 两个术语，URI 全称为 UniformResource Identifier，即统一资源标志符，URL 全称为 Universal Resource Locator，即统一资源定位符。例如https://github.com/favicon.ico，我们用 URL/URI 来唯一指定了它的访问方式。这其中包括了访问协议 https、访问路径/即根目录，资源名称 favicon.ico，通过这样的一个链接我们便可以从互联网上找到这个资源，这就是 URL/URI。URL是URI的子集。也就是说每个 URL 都是 URI，但不是每个 URI 都是 URL。URI 还包括一个子类叫做 URN，它的全称为 Universal Resource Name，即统一资源名称。URN 只命名资源而不指定如何定位资源，如 urn:isbn:0451450523，它指定了一本书的 ISBN，可以唯一标识这一本书，但是没有指定到哪里定位这本书，这就是 URN，URL、URN、URI 的关系可以用图 2-1 表示如下：看不懂的话也，也没关系，我们只需知道URL是什么就可以了，现今几乎所有的URI也是URL. 超文本超文本。超文本英文名称叫做 Hypertext，我们在浏览器里面看到的网页就是超文本解析而成的，其网页源代码是一系列 HTML 代码，里面包含了一系列标签，如 img 显示图片，p 指定显示段落等，浏览器解析这些标签后便形成了我们平常看到的网页，而这网页的源代码 HTML 就可以称作超文本。我们可以用浏览器的开发者工具查看网页源代码，这些源代码都是超文本，如图 2-2 所示：当然我们没必要看懂这些代码是做什么的，毕竟我们不是前端工程师，但是如果了解的话会更好。 HTTPS 和HTTP我们访问网页时会发现有的是http打头，有的却是https,有没有想过为什么，它们的区别在哪里？http 或 https，这个就是访问资源需要的协议类型，有时我们还会看到 ftp、sftp、smb 开头的 URL，那么这里的 ftp、sftp、smb 都是指的协议类型。在爬虫中，我们抓取的页面通常就是 http 或 https 协议的。HTTP 的全称是 Hyper Text Transfer Protocol，中文名叫做超文本传输协议，HTTP 协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证传送高效而准确地传送超文本文档。HTTP 由万维网协会（World Wide Web Consortium）和 Internet 工作小组IETF（Internet Engineering Task Force）共同合作制定的规范，目前广泛使用的是 HTTP 1.1 版本。而HTTPS则是为了安全性在HTTP下加入了SSL层，简称HTTPS。HTTPS 的安全基础是 SSL，因此通过它传输的内容都是经过 SSL 加密的，它的主要作用可以分为两种：1：是建立一个信息安全通道，来保证数据传输的安全。2：确认网站的真实性，凡是使用了 https 的网站，都可以通过点击浏览器地址栏的锁头标志来查看网站认证之后的真实信息，也可以通过 CA 机构颁发的安全签章来查询。随着大数据时代的发展越来越多的网站往HTTPS方向发展，为了安全。 但是有的网站即使是https协议仍然会被提示不安全，比如12306：这是因为它的CA证书是中国铁道部自己颁发给自己的，而这个证书是不被官方机构认可的，所以这里证书验证就不会通过而提示这样的话，但是实际上它的数据传输依然是经过 SSL 加密的。我们如果要爬取这样的站点就需要设置忽略证书的选项，否则会提示 SSL链接错误。 HTTP请求过程，我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容，实际上这个过程是浏览器向网站所在的服务器发送了一个 Request，即请求，网站服务器接收到这个 Request 之后进行处理和解析，然后返回对应的一个 Response，即响应，然后传回给浏览器，Response里面就包含了页面的源代码等内容，浏览器再对其进行解析便将网页呈现了出来，我们可以利用开发者工具查看这一过程， Chrome 浏览器的开发者模式下的 Network 监听组件来做下演示，它可以显示访问当前请求网页时发生的所有网络请求和响应。第一列 Name，即 Request 的名称。一般会用URL的最后一部分内容当做名称。第二列 Status，即 Response 的状态码。这里显示为 200，代表 Response 是正常的，通过状态码我们可以判断发送了 Request 之后是否得到了正常的 Response。第三列 Type，即 Request 请求的文档类型。这里为 document，代表我们这次请求的是一个 HTML 文档，内容就是一些 HTML 代码。第四列 Initiator，即请求源。用来标记 Request 是由哪个对象或进程发起的。第五列 Size，即从服务器下载的文件和请求的资源大小。如果是从缓存中取得的资源则该列会显示 from cache。第六列 Time，即发起 Request 到获取到 Response 所用的总时间。第七列 Timeline，即网络请求的可视化瀑布流。我们点击这个条目即可看到其更详细的信息，如图 2-7 所示：首先是 General 部分，Request URL 为 Request 的 URL，Request Method 为请求的方法，Status Code 为响应状态码，Remote Address 为远程服务器的地址和端口，Referrer Policy 为 Referrer 判别策略。再继续往下看可以看到有一个 Response Headers 和一个 Request Headers，这分别代表响应头和请求头，请求头里面带有许多请求信息，例如浏览器标识、Cookies、Host 等信息，这是 Request 的一部分，服务器会根据请求头内的信息判断请求是否合法，进而作出对应的响应，返回 Response，那么在图中看到的 Response Headers 就是 Response 的一部分，例如其中包含了服务器的类型、文档类型、日期等信息，浏览器接受到 Response 后，会解析响应内容，进而呈现网页内容。 RequestRequest，即请求，由客户端向服务端发出。可以将 Request 划分为四部分内容：Request Method、Request URL、Request Headers、Request Body，即请求方式、请求链接、请求头、请求体。 Request Method请求方式，请求方式常见的有两种类型，GET 和 POST。我们在浏览器中直接输入一个 URL 并回车，这便发起了一个 GET 请求，请求的参数会直接包含到 URL 里，例如百度搜索 Python，这就是一个 GET 请求，链接为：https://www.baidu.com/s?wd=Python，URL 中包含了请求的参数信息，这里参数 wd 就是要搜寻的关键字。POST 请求大多为表单提交发起，如一个登录表单，输入用户名密码，点击登录按钮，这通常会发起一个 POST 请求，其数据通常以 Form Data 即表单的形式传输，不会体现在 URL 中。GET 和 POST 请求方法有如下区别：GET 方式请求中参数是包含在 URL 里面的，数据可以在 URL 中看到，而 POST 请求的 URL 不会包含这些数据，数据都是通过表单的形式传输，会包含在 Request Body 中。GET 方式请求提交的数据最多只有 1024 字节，而 POST 方式没有限制。所以一般来说，网站登录验证的时候，需要提交用户名密码，这里包含了敏感信息，使用GET方式请求的话密码就会暴露在URL里面，造成密码泄露，所以这里最好以POST方式发送。文件的上传时，由于文件内容比较大，也会选用POST方式。 我们平常遇到的绝大部分请求都是 GET 或 POST 请求，另外还有一些请求方式，如 HEAD、PUT、DELETE、OPTIONS、CONNECT、TRACE，我们简单将其总结如下： Request Headers请求头，用来说明服务器要使用的附加信息，比较重要的信息有 Cookie、Referer、User-Agent 等，下面将一些常用的头信息说明如下：Accept，请求报头域，用于指定客户端可接受哪些类型的信息。Accept-Language，指定客户端可接受的语言类型。Accept-Encoding，指定客户端可接受的内容编码。Host，用于指定请求资源的主机 IP 和端口号，其内容为请求 URL 的原始服务器或网关的位置。从 HTTP 1.1 版本开始，Request 必须包含此内容。Cookie，也常用复数形式 Cookies，是网站为了辨别用户进行 Session 跟踪而储存在用户本地的数据。Cookies 的主要功能就是维持当前访问会话，例如我们输入用户名密码登录了某个网站，登录成功之后服务器会用 Session 保存我们的登录状态信息，后面我们每次刷新或请求该站点的其他页面时会发现都是保持着登录状态的，在这里就是 Cookies 的功劳，Cookies 里有信息标识了我们所对应的服务器的 Session 会话，每次浏览器在请求该站点的页面时都会在请求头中加上 Cookies 并将其发送给服务器，服务器通过 Cookies 识别出是我们自己，并且查出当前状态是登录的状态，所以返回的结果就是登录之后才能看到的网页内容。Referer，此内容用来标识这个请求是从哪个页面发过来的，服务器可以拿到这一信息并做相应的处理，如做来源统计、做防盗链处理等。User-Agent，简称 UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、浏览器及版本等信息。在做爬虫时加上此信息可以伪装为浏览器，如果不加很可能会被识别出为爬虫。Content-Type，即 Internet Media Type，互联网媒体类型，也叫做 MIME 类型，在 HTTP 协议消息头中，使用它来表示具体请求中的媒体类型信息。例如 text/html 代表 HTML 格式，image/gif 代表 GIF 图片，application/json 代表 Json 类型，更多对应关系可以查看此对照表：http://tool.oschina.net/commons。因此，Request Headers 是 Request 等重要组成部分，在写爬虫的时候大部分情况都需要设定 Request Headers。还有Requests Body这里不赘述，一般承载的内容是 POST 请求中的 Form Data，即表单数据，而对于 GET 请求 Request Body 则为空。 Response响应状态码，此状态码表示了服务器的响应状态，如 200 则代表服务器正常响应，404 则代表页面未找到，500 则代表服务器内部发生错误。在爬虫中，我们可以根据状态码来判断服务器响应状态，如判断状态码为 200，则证明成功返回数据，再进行进一步的处理，否则直接忽略。其他还有一些常见的比如：404：服务器找不到请求的网页。504：网关超时其他的自行查找吧。 Response Headers响应头，其中包含了服务器对请求的应答信息，如 Content-Type、Server、Set-Cookie 等，下面将一些常用的头信息说明如下：Date，标识 Response 产生的时间。Last-Modified，指定资源的最后修改时间。Content-Encoding，指定 Response 内容的编码。Server，包含了服务器的信息，名称，版本号等。Content-Type，文档类型，指定了返回的数据类型是什么，如text/html 则代表返回 HTML 文档，application/x-javascript 则代表返回 JavaScript 文件，image/jpeg 则代表返回了图片。Set-Cookie，设置Cookie，Response Headers 中的 Set-Cookie即告诉浏览器需要将此内容放在 Cookies 中，下次请求携带 Cookies 请求。Expires，指定 Response 的过期时间，使用它可以控制代理服务器或浏览器将内容更新到缓存中，如果再次访问时，直接从缓存中加载，降低服务器负载，缩短加载时间。 Resposne Body即响应体，最重要的当属响应体内容了，响应的正文数据都是在响应体中，如请求一个网页，它的响应体就是网页的 HTML 代码，请求一张图片，它的响应体就是图片的二进制数据。所以最主要的数据都包含在响应体中了，我们做爬虫请求网页后要解析的内容就是解析响应体，如图 2-9 所示我们在浏览器开发者工具中点击 Preview，就可以看到网页的源代码，这也就是响应体内容，是解析的目标。我们在做爬虫时主要解析的内容就是 Resposne Body，通过 Resposne Body 我们可以得到网页的源代码、Json 数据等等，然后从中做相应内容的提取。 以上便是 Response 的组成部分。我们了解了 HTTP 的基本原理，通过如上描述，我们应该对访问网页背后的请求和响应过程有了大体的认识，本节涉及到的知识点需要好好掌握，在后面分析网页请求的时候会经常用到。 代理ip为什么要用代理ip有时候你会发现自己辛辛苦苦写好的爬虫程序正在运行，正当自己心底沾沾自喜的时候，突然报错，抓取不到网页了！这是为什么呢，自己程序也没错误啊！ 打开网页一看发现提示自己的ip被封了，说我们的访问频率太高了，因此把我们的ip封禁了，这也是常见的反爬虫措施之一。 为什么要封禁我们的ip呢，因为我们的爬虫多次对这个网页请求，加大了服务器的负担，为了不影响正常用户的使用，也为了不让自己辛辛苦苦的数据被你无情的拿走，直接禁止你访问这个网页。 一般是单位时间内设置一个阈值，如果超过这个访问频率，那么直接say goodbye.那么我们甘心被它无情的封禁吗？显然不可能，既然你根据单位时间内的阈值判断是否为爬虫，那么我可以设置缓冲时间来间歇访问，但这样显然会拖慢我们的时间，还是更换代理ip进行访问比较方便，你封了一个我就换个ip继续访问。 代理的基本原理我们常称呼的代理实际上指的就是代理服务器，英文叫做 Proxy Server，它的功能是代理网络用户去取得网络信息。形象地说，它是网络信息的中转站。在我们正常请求一个网站时，是发送了 Request 给 Web 服务器，Web 服务器把 Response 传回给我们。如果设置了代理服务器，实际上就是在本机和服务器之间搭建了一个桥，此时本机不是直接向 Web 服务器发起请求，而是向代理服务器发出请求， Request 会发送给代理服务器，然后由代理服务器再发送给 Web 服务器，然后由代理服务器再把 Web 服务器返回的 Response 转发给本机，这样我们同样可以正常访问网页，但这个过程 Web 服务器识别出的真实的 IP 就不再是我们本机的 IP 了，就成功实现了 IP 伪装，这就是代理的基本原理。 通俗的讲就是找了一个不收差价的中间商，我们不直接对话，通过这个中间商进行沟通。 代理的作用我们可以简单列举如下： 突破自身 IP 访问限制，访问一些平时不能访问的站点。比如：YouTube，推特……….. 访问一些单位或团体内部资源，如使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类 FTP 下载上传，以及各类资料查询共享等服务。 比如我们学校的内网，在某些时候是不对外界开放的，只能通过校园网进行访问。 提高访问速度，通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时， 则直接由缓冲区中取出信息，传给用户，以提高访问速度。 隐藏真实 IP，上网者也可以通过这种方法隐藏自己的 IP，免受攻击，对于爬虫来说，我们用代理就是为了隐藏自身 IP，防止自身的 IP 被封锁。黑客也一般会使用很多代理，尽力隐藏自己的真实地址（为了人身安全）。 代理的类别匿名性分类列举如下： 高度匿名代理，高度匿名代理会将数据包原封不动的转发，在服务端看来就好像真的是一个普通客户端在访问，而记录的 IP 是代理服务器的 IP。 普通匿名代理，普通匿名代理会在数据包上做一些改动，服务端上有可能发现这是个代理服务器，也有一定几率追查到客户端的真实 IP。代理服务器通常会加入的 HTTP 头有 HTTP_VIA 和 HTTP_X_FORWARDED_FOR。 透明代理，透明代理不但改动了数据包，还会告诉服务器客户端的真实 IP。这种代理除了能用缓存技术提高浏览速度，能用内容过滤提高安全性之外，并无其他显著作用，最常见的例子是内网中的硬件防火墙。 间谍代理，间谍代理指组织或个人创建的，用于记录用户传输的数据，然后进行研究、监控等目的代理服务器。 协议分类 FTP 代理服务器，主要用于访问 FTP 服务器，一般有上传、下载以及缓存功能，端口一般为 21、2121 等。 HTTP 代理服务器，主要用于访问网页，一般有内容过滤和缓存功能，端口一般为 80、8080、3128 等。 SSL/TLS 代理，主要用于访问加密网站，一般有 SSL 或 TLS 加密功能（最高支持 128 位加密强度），端口一般为 443。 RTSP 代理，主要用于 Realplayer 访问 Real 流媒体服务器，一般有缓存功能，端口一般为 554。 Telnet代理，主要用于 telnet 远程控制（黑客入侵计算机时常用于隐藏身份），端口一般为23。 POP3/SMTP 代理，主要用于 POP3/SMTP 方式收发邮件，一般有缓存功能，端口一般为 110/25。 SOCKS代理，只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能，端口一般为1080。SOCKS 代理协议又分为 SOCKS4 和 SOCKS5，SOCKS4 协议只支持 TCP，而 SOCKS5 协议支持 TCP 和 UDP，还支持各种身份验证机制、服务器端域名解析等。简单来说，SOCK4能做到的SOCKS5都可以做到，但SOCKS5能做到的SOCK4不一定能做到。 我们可以常见的有这些： 网上的免费代理，最好使用高匿代理，使用前抓取下来筛选一下可用代理，也可以进一步维护一个代理池。 付费代理服务，互联网上存在许多代理商，可以付费使用，质量比免费代理好很多。ADSL拨号，拨一次号换一次 IP，稳定性高，也是一种比较有效的解决方案。 参考自 ：https://germey.gitbooks.io/python3webspider/https://zh.wikipedia.org/wiki/%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网易云音乐评论]]></title>
    <url>%2F2018%2F10%2F18%2F%E7%BD%91%E6%98%93%E4%BA%91%E8%AF%84%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[本文是为了爬取遍地情圣的网易评论区 用到的知识主要有:requestes,beautifulsoup,PyCryptodome等。 ###加密的网页首先我们随机点开一首歌，用requests.get()获取其源码，结果我们发现好多信息是经过加密的，我们不能提取到有效的信息，贴个代码演示一下。 &lt;a href="/song?id=${x.id}">&lt;b title="${x.name|escape}{if alia} - (${alia|escape}){/if}">${soil(x.name)}&lt;/b>&lt;/a>{if alia}&lt;span title="${alia|escape}" class="s-fc8"> - (${soil(alia)})&lt;/span>{/if} {if x.mvid>0} &lt;span data-res-id="${x.id}" data-res-action="mv" title="播放mv" class="mv">MV&lt;/span> {/if} &lt;/span> &lt;/div> &lt;/div> &lt;div class="opt hshow"> &lt;a class="u-icn u-icn-81 icn-add" href="javascript:;" title="添加到播放列表" hidefocus="true" data-res-type="18" data-res-id="${x.id}" data-res-action="addto" {if from}data-res-from="${from.fid}" data-res-data="${from.fdata}"{/if}>&lt;/a> &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="fav" class="icn icn-fav" title="收藏">&lt;/span> &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="share" data-res-name="${x.name}" data-res-author="{list x.artists as art}${art.name}{if art_index&lt;x.artists.length-1}/{/if}{/list}" {if x.album}data-res-pic="${x.album.picUrl}"{/if} class="icn icn-share" title="分享">分享&lt;/span> &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="download" class="icn icn-dl" title="下载">&lt;/span> {if canDel} &lt;span data-res-id="${x.id}" data-res-type="18" data-res-action="delete" class="icn icn-del" title="删除">删除&lt;/span> {/if} 我们可以清楚的看到，我们需要提取的信息都被加密过。无法通过普通的办法获得我们想要的内容。 两种思路这时我们有两种思路：1：使用selenium模拟浏览器的操作进行爬取。此方法我们不在这篇文章里讲述，想尝试的同学可以自己尝试。2：鉴于该数据采取ajax，我们通过访问该网址获取其json字符串，直接完成其信息的采集。我们先按思路二进行下去。 获取第一页数据首先在加载的一堆数据中排查得到我们需要的网址，如下： Request URL: https://music.163.com/weapi/v1/resource/comments/R_SO_4_531295576?csrf_token= Request Method: POST Status Code: 200 OK Remote Address: 127.0.0.1:1080 Referrer Policy: no-referrer-when-downgrade Accept: */* Accept-Encoding: gzip, deflate, br Accept-Language: zh-CN,zh;q=0.9,en;q=0.8 Connection: keep-alive Content-Length: 482 Content-Type: application/x-www-form-urlencoded Cookie: 过长就不贴了。 Host: music.163.com Origin: https://music.163.com Referer: https://music.163.com/song?id=531295576 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36 Form Ddta: params: +0YydG970lh0RFa4CtT3YQEDyAB+YtGNJ+UbomdqEnY+TemTKHVbA3vJIX3NKgm07UOy+J2sKcSTM0smfpKbOA4Dm8TJcxbWZrVnjeVl/zE3z3PX21iD84D5Qt50oLrZv8Gug+LqS/y1JWBWiq6DZ0ZaZZEG8aYRYF2bwnPxX9v55DxZJmiubjteCc5lMs0S encSecKey: 762617ad7682a61121e378406d57ffd5aede0f3621e18d1584dab1eca7fdf3ca3d842d882111609a4ada5d3c24973450d1a8553ff96641f144a6de81688ae2c0a5798f8c23fa38e16139999692b6c91150b39c25c014ee6a2005e821485c3a6eab822db16f0397be0c54e43993cc34eae70c68739d19d0eb69e7ca86b16b585a 我们可以看出他是一个post请求，通过多次测试我们发现网址中那个数字会有变化。其他则保持不变，这串数字也很简单的可以发现就是这首歌在网易云的id，我们需要先爬取这个id之后才能进行下一步的操作(以热歌榜为例）。上代码说话。 import requests import os import re from requests import ConnectionError from requests import Timeout headers = { "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36" } def get_index(url): try: response = requests.get(url,headers = headers,timeout = 5) if response.status_code == 200: response.encoding = 'utf8' return response.text else: print("Error",response.status_code) except ConnectionError as e: print("Error",e.args) except Timeout as t: print("Error",t.args) def parse_url(html): hot_list = re.findall(r'&lt;ul class="f-hide">&lt;li>&lt;a href="/song\?id=\d*?">.*&lt;/a>&lt;/li>&lt;/ul>',html) hot_list = hot_list[0] hot_music_id = re.findall(r'&lt;li>&lt;a href="/song\?id=(\d*?)">.*?&lt;/a>&lt;/li>',hot_list) hot_music_name = re.findall(r'&lt;li>&lt;a href="/song\?id=\d*?">(.*?)&lt;/a>&lt;/li>',hot_list) return hot_music_name,hot_music_id if __name__ == "__main__": url = "https://music.163.com/discover/toplist?id=3778678" html = get_index(url) if html: hot_music_name,hot_music_id = parse_url(html) 我用正则表达式提取出其id和名字，既然id到了那么下面应该很简单的吧。答案是这样吗？我们继续看： 构造参数请求数据里有两个参数，params 和 encSecKey。如果我们只爬取该歌曲的第一页评论（或者说热评）的话，那么我们只要把这两个参数复制然后构造post请求，可以简单的做到这一点。 def get_html(id): data ={ "params": "dPn4YKam1ALLghjJadhyim4G05a1atAbF7ECvh5EGXihbxOV1i+TRS2oNZv+jR8H6nZ0DSoff6PQQ9mEnocZIv8D5ispn9aLDlta+vylq2wUnJQYLR0KkoQFYcXQ+VOvbmJyLK1acsZ/GuCkP+XFbr3h9WtEAXu3sxfn0DKFO06eNMufFrUHTmw7zyVFp64zYAHK4jIvkSTAkkVYqMu2IbwYG4coJ6wScE1DwKyRQlE=", "encSecKey": "ca034db233499f555cd0009cf367f35093381ded33765fb6026c09c333a20b41d87bcd71504fc87159d55f6b2308343b0d443b8c68c575c1b6e6c37c98fdb571e4a0f61beb292ba4847840ba52f4d0f1c3169e3a2492c00a2cc7b4664a5eb63c6c06be9d90b9a0dc0c305aa8204820225ab5aeace9c2d4ee0dd628a5a022cdba" } url = BASE_URL + id + "?csrf_token=" try: response = requests.post(url,headers=headers,data=data) if response.status_code == 200: return response.json() else: print("Error") except ConnectionError as e: print("Error",e.args) def save_parse(name,data): data = data['hotComments'] if not os.path.exists("Comments"): os.mkdir("Comments") file_path = "{}/Hot.txt".format("Comments") with open(file_path,"a",encoding="utf-8") as f: f.write(name + "\n") for contents in data: f.write(str(num) +":"+ contents['content'] + "\n") f.write("\n" + "***************************************************" + "\n") print("Success",name) 但是我们仅仅满足于热评吗？？？网易云的情圣们可能还隐藏在里面，那么我们继续采坑吧。 破译加密算法我们点击评论的下一页，通过开发者工具我们可以发现网址丝毫不变，其他也基本不变，那么我们的数据是怎么改变的呢，post传输的数据为罪魁祸首，那么这些post数据是怎么变化的呢，通过不断地寻找我们锁定了一个js脚本，我们发现这串数据来自于它，并且是加密过的（好难受），加密函数如下： !function() { function a(a) { var d, e, b = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789", c = ""; for (d = 0; a > d; d += 1) e = Math.random() * b.length, e = Math.floor(e), c += b.charAt(e); return c } function b(a, b) { var c = CryptoJS.enc.Utf8.parse(b) , d = CryptoJS.enc.Utf8.parse("0102030405060708") , e = CryptoJS.enc.Utf8.parse(a) , f = CryptoJS.AES.encrypt(e, c, { iv: d, mode: CryptoJS.mode.CBC }); return f.toString() } function c(a, b, c) { var d, e; return setMaxDigits(131), d = new RSAKeyPair(b,"",c), e = encryptedString(d, a) } function d(d, e, f, g) { var h = {} , i = a(16); return h.encText = b(d, g), h.encText = b(h.encText, i), h.encSecKey = c(i, e, f), h } function e(a, b, d, e) { var f = {}; return f.encText = c(a + e, b, d), f } window.asrsea = d, window.ecnonasr = e }(); window.asrsea函数就是那个加密的罪魁祸首。也就是上面的d函数. 可以看出它采用CryptoJS加密,我们可以通过PyCryptodome(python3用户），如果你是python2的话，请安装这个pycrypto，pycrypto已经宣布永久停止维护了，所以请移步PyCryptodome吧，https://github.com/Legrandin/pycryptodome。 那么我们是如何发现这个函数的呢，Chrome开发者控制面板–source–点击 Event Listener Breakpoints–勾选XHR–点击重新加载–然后点击 Step over next function call的那个图标，就这样单步调试过去，就能找到那个函数。。然后我们打开Chrome的调试工具，把断点设在12973行我们可以发现上面的参数（格式化js语句点击下面出现的{}标志即可），既然找到了加密函数和相应的参数，那么我们开始用py模仿破解吧，经过多次翻页我们发现只有{rid: “R_SO_4_574566207”, offset: “40”, total: “false”, limit: “20”, csrf_token: “”}{rid: “R_SO_4_574566207”, offset: “60”, total: “false”, limit: “20”, csrf_token: “”}可以看出只有offset参数是变化的，limit参数经过分析是控制每页评论数的，限制每页20条，其他则是固定不变的，结合上面的window.asrsea()函数，我们构造出加密字符串，完成post请求进而得到我们的评论数据。 python构造加密回到我们的加密函数上，我们发现它经过AES加密和RSA加密。 function d(d, e, f, g) { var h = {} , i = a(16); return h.encText = b(d, g), #AES加密 h.encText = b(h.encText, i), #AES加密 h.encSecKey = c(i, e, f), #RSA加密 h } 不懂这些加密函数什么意思，自学啊，还是先模仿吧。用到我们上面写到的PyCryptodome库，进行Crypto加密。我还是贴代码吧。。 #导入这些第三方库 import requests import math import random import base64 import codecs from Crypto.Cipher import AES 首先我们需要生成长度为16的随机字符串,这里我们仿照上面的javascript的实现,用Python生成16位长的随机字符串: def generate_random_strs(length): string = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789" # 控制次数参数i i = 0 # 初始化随机字符串 random_strs = "" while i &lt; length: e = random.random() * len(string) # 向下取整 e = math.floor(e) random_strs = random_strs + list(string)[e] i = i + 1 return random_strs AES加密的模式是AES.MODE_CBC,初始化向量iv=’0102030405060708′,具体的AES加密 # AES加密 def AESencrypt(msg, key): # 如果不是16的倍数则进行填充(paddiing) padding = 16 - len(msg) % 16 # 这里使用padding对应的单字符进行填充 msg = msg + padding * chr(padding) # 用来加密或者解密的初始向量(必须是16位) iv = '0102030405060708'.encode('utf-8') key = key.encode('utf-8') cipher = AES.new(key, AES.MODE_CBC, iv) # 加密后得到的是bytes类型的数据 encryptedbytes = cipher.encrypt(msg.encode('utf-8')) # 使用Base64进行编码,返回byte字符串 encodestrs = base64.b64encode(encryptedbytes) # 对byte字符串按utf-8进行解码 enctext = encodestrs.decode('utf-8') return enctext RSA加密.首先我简单介绍一下RSA的加密过程.在RSA中,明文,密钥和密文都是数字.RSA的加密过程可以用下列的公式来表达,这个公式非常的重要,你只有理解了这个公式,才能用Python实现RSA加密 密文 = 明文^E mod N (RSA加密) RSA的密文是对代表明文的数字的E次方求mod N 的结果, 通俗的讲就是将明文和自己做E次乘法,然后将其结果除以N 求余数,这个余数就是密文. # RSA加密 def RSAencrypt(randomstrs, key, f): # 随机字符串逆序排列 string = randomstrs[::-1] # 将随机字符串转换成byte类型数据 text = bytes(string, 'utf-8') seckey = int(codecs.encode(text, encoding='hex'), 16)**int(key, 16) % int(f, 16) return format(seckey, 'x').zfill(256) RSA加密后得到的字符串长为256,如果不够长则进行填充(不足部分在左侧添0).然后就是获取那两个参数。 def get_params(page): # msg也可以写成msg = {"offset":"页面偏移量=(页数-1) * 20", "limit":"20"},offset和limit这两个参数必须有(js) # limit最大值为100,当设为100时,获取第二页时,默认前一页是20个评论,也就是说第二页最新评论有80个,有20个是第一页显示的 # msg = '{"rid":"R_SO_4_1302938992","offset":"0","total":"True","limit":"100","csrf_token":""}' # 偏移量 offset = (page-1) * 20 # offset和limit是必选参数,其他参数是可选的,其他参数不影响data数据的生成 msg = '{"offset":' + str(offset) + ',"total":"True","limit":"20","csrf_token":""}' key = '0CoJUm6Qyw8W8jud' f = '00e0b509f6259df8642dbc35662901477df22677ec152b5ff68ace615bb7b725152b3ab17a876aea8a5aa76d2e417629ec4ee341f56135fccf695280104e0312ecbda92557c93870114af6c9d05c4f7f0c3685b7a46bee255932575cce10b424d813cfe4875d3e82047b97ddef52741d546b8e289dc6935b3ece0462db0a22b8e7' e = '010001' enctext = AESencrypt(msg, key) # 生成长度为16的随机字符串 i = generate_random_strs(16) # 两次AES加密之后得到params的值 encText = AESencrypt(enctext, i) # RSA加密之后得到encSecKey的值 encSecKey = RSAencrypt(i, e, f) return encText, encSecKey 下面构造表单不再赘述，所有参数都已得到。如果想用selenium的大佬，可以自己尝试一下。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k近邻算法]]></title>
    <url>%2F2018%2F10%2F13%2Fk%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本文是根据机器学习实战那本书学习的部分总结 k近邻算法简介简单的来说就是根据测量不同特征值之间的距离进而对数据集进行分类。 该算是原理是根据我们已知的特征值对应关系，对我们想要分类的新数据进行判断分类。比如我们想判断一个电影的题材那么我们就可以提取一定的特征值，电影中惊悚片段的次数等等，然后用该算法构造程序，自动化分。存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。 k近邻算法的流程1收集数据：爬虫或者一些已存在的数据集2准备数据：结构化数据格式3分析数据：可用任何方法4训练算法：（不适用于k近邻）5测试算法：计算成功率6使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理 导入数据集请看代码：1：导入数据 from numpy import * import operator def createDataSet(): group = array([[1,1],[3.1],[1,3],[3,3]) labels = ['A',"B","C","D"] return group,labels 定义一个标准数据集（每组包含两个数据）和标签，一一对应。我们将[1,1]定义为类A,依次类推。 进行距离运算，预测类型def classify0(inX, dataSet,labels,k): dataSetsize = dataSet.shape[0] #计算训练数据集的行数 diffMat = tile(inX,(dataSetsize,1) - dataSet) sqDiffMat = diffMat**2 sqDistance = sqDiffMat.sum(axis) #将一轴上的数据相加 distance = sqDistance**0.5 sorteDistIndicies = distance.argsort() #返回distances中元素从小到大排序后的索引值 classCount = {} for i in range(k): voteIlable = labels[sorteDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) #key=operator.itemgetter(1)根据字典的值进行排序，reverse = True 以降序排列 return sortedClassCount[0][0] 这样我们就可以返回次数出现最高的标签进而输出预测值，但是我们无法判断这个分类是否正确，因此我们需要测试一下。 海伦约会-栗子她把这些数据存放在文本文件datingTestSet.txt中海伦的样本主要包含以下3种特征： 每年获得的飞行常客里程数 玩视频游戏所耗时间百分比 每周消费的冰淇淋公升数首先py读取收集到的txt文件数据，使其可以被使用 def file2matrix(filename): fr = open(filename) arrayoLine = fr.readlines() numberoflines = len(arrayoLine) returnMat = zeros((numberoflines,3)) classLabelVectors = [] index = 0 for line in arrayoLine: line = line.strip() #去除空白 listFormLine = line.split('\t') #以制表符为标志切片 returnMat[index,:] = listFormLine[0:3] if listFromLine[-1] == 'didntLike': classLabelVector.append(1) elif listFromLine[-1] == 'smallDoses': classLabelVector.append(2) elif listFromLine[-1] == 'largeDoses': classLabelVector.append(3) index+=1 return returnMat,classLabelVector 图形化展示返回读取成功的数据集，和标签集合集合。先进行图形化分析一下。 import matplotlib import matplotlib.pyplot as plt fig = plt.figure() #创建一个画布 ax = fig.add_subplot(111) ax.scatter(datingDataMat[:,1],datingDataMat[:,2],15.0*array(datingLabels),15.0*array[datingLabels]) #以特征值一和二为数据画图 plt.show() 图略，我们发现一些数字值相差太大影响我们的结果，因此我们进行归一化数值： def autoNorm(dataSet): minVals = dataSet.min(0) #min()中0对应列，1对应行。不输入测全部数据的最小值 maxVals = dataSet.max(0) ranges = maxVals - minVals normDataSet = zeros(shape(dataSet)) m = dataSet.shape[0] normDataSet = dataSet - tile(minVals,(m,1)) normDataSet = normDataSet/(tile(ranges,(m,1))) return normDataSet,ranges,minVals 将最大的数值当作1，最小的当作0，其他等比例压缩。通常我们只提供已有数据的90%作为训练样本来训练分类器，而使用其余的10%数据去测试分类器，检测分类器的正确率。本书后续章节还会介绍一些高级方法完成同样的任务，这里我们还是采用最原始的做法。需要注意的是，10%的测试数据应该是随机选择的，由于海伦提供的数据并没有按照特定目的来排序，所以我们可随意选择10%数据而不影响其随机性。 计算错误率代码里我们定义一个计数器变量，每次分类器错误地分类数据，计数器就加1，程序执行完成之后计数器的结果除以数据点总数即是错误率进入正题测试预测结果： def datingClassTest(): horatio = 0.10 datingDataMat,datingLabels = file2matrix('datingTestSet,txt') normMat,ranges,minVals = autoNorm(datingDataMat) m = normMat.shape[0] numTestVecs = int(m*horatio) errCount = 0.0 for i in range(numTestVecs): classifreResult = classify0(normMat[i,:],normMat[numTestVecs,:],datingLabels[numTestVecs:m],3) print("分类结果:%s\t真实类别:%d" % (classifierResult, datingLabels[i])) if classifierResult != datingLabels[i]: errorCount += 1.0 print("错误率:%f%%" % (errorCount / float(numTestVecs) * 100)) 一个简单的分类系统就做好了，我们需要优化一下。让海伦找到自己理想的伴侣 def calssifyPerson(): resultList=['not at all','in small doses','in large doses'] percentTats=float(input("percentage of time spent playing video games?")) ffMiles=float(input("frequent flier miles earned per year?")) iceCream=float(input("liters of ice cream consumed per year?")) datingDataMat,datingLabels=file2matrix('datingTestSet2.txt')#原书没有2 normMat, ranges, minVals = autoNorm(datingDataMat) inArr=array([ffMiles,percentTats,iceCream]) classifierResult=classify0((inArr-minVals)/ranges,normMat,datingLabels,3) print("You will probably like this person:", resultList[classifierResult-1]) 我们输入对应的数据，代码帮我们输出他适不适合海伦（错过了真命天子就嘿嘿了） 我们还可以通过k近邻算法，做一个手写识别系统注图像已经经过处理为黑白图像（由0，1）字符组成。 0000000000001111000000000000000000000000000111111000000000000000000000000011111111000000000000000000000000011111111110000000000000000000001111111111110000000000000000000011111111111110000000000000000001111110111111100000000000000000011111000111111000000000000000000111110000111111000000000000000011111000000111111000000000000001111100000001111110000000000000011111000000000111100000000000000111110000000001111100000000000001111100000000011111000000000000011111000000000111110000000000000111110000000001111100000000000011111100000000011111000000000000011111000000000011110000000000000111111000000000111100000000000001111100000000001111000000000000011111000000000111110000000000000111110000000001111100000000000001111110000000011111000000000000011111100000011111100000000000000011111000000111111000000000000000111111000011111100000000000000000111110000111110000000000000000001111111111111100000000000000000011111111111111000000000000000000011111111111000000000000000000000001111111111000000000000000000000000111110000000000000 将数据转化为测试向量： def img2Vector(filename): returnVect = zeros((1,1024)) fr = open(filenaem) for i in range(32): lineStr = fr.readline() for j in range(32): returnVect[0,32*i+j] = int[lineStr[j]] return returnVect 我们将这些数据输入到分类器，检测分类器的执行效果。程序清单2-6所示的自包含函数handwritingClassTest()是测试分类器的代码，将其写入kNN.py文件中。在写入这些代码之前，我们必须确保将from os import listdir写入文件的起始部分，这段代码的主要功能是从os模块中导入函数listdir，它可以列出给定目录的文件名。 import os,sys def handwritingClassTest(): hwLabels=[] trainingFileList=os.listdir('trainingDigits')#修改 import os 这里加上os. m=len(trainingFileList) trainingMat=zeros((m,1024)) #定义文件数x每个向量的训练集 for i in range(m): fileNameStr=trainingFileList[i] fileStr=fileNameStr.split('.')[0]#解析文件 classNumStr=int(fileStr.split('_')[0])#解析文件名 hwLabels.append(classNumStr)#存储类别 trainingMat[i,:]=img2vector('trainingDigits/%s'%fileNameStr) #访问第i个文件内的数据 #测试数据集 testFileList=os.listdir('testDigits') errorCount=0.0 mTest=len(testFileList) for i in range(mTest): fileNameStr=testFileList[i] fileStr=fileNameStr.split('.')[0] classNumStr=int(fileStr.split('_')[0])#从文件名中分离出数字作为基准 vectorUnderTest=img2vector('testDigits/%s'%fileNameStr)#访问第i个文件内的测试数据，不存储类 直接测试 classifierResult=classify0(vectorUnderTest,trainingMat,hwLabels,3) print("the classifier came back with: %d,the real answer is: %d" %(classifierResult,classNumStr)) if(classifierResult!=classNumStr): errorCount+=1.0 print("\nthe total number of errors is: %d" % errorCount) print("\nthe total rate is:%f"% (errorCount/float(mTest))) 两个函数就完成了这个识别系统，代码和数据集在github上都有，这里不在贴。]]></content>
      <categories>
        <category>Python</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析库Beautiful-Soup]]></title>
    <url>%2F2018%2F09%2F28%2Fclass2%2F</url>
    <content type="text"><![CDATA[本文简单的记录一下自己对于Beautifulsoup库的理解 Beautiful Soup简介Beautoful Soup借助网页的结构和属性来进行解析网页，比正则表达式要简便的多。省去很多繁琐的提取工作，大大提高了解析效率。 1：首先确保我们已经安装了Beautiful Soup 和 lxml. lxml的安装建议不要直接用pip install lxml，会遇到难以消除的错误，从官网直接下载.whl文件（注意和你的Python版本匹配）然后进行安装，亲测方便快捷。 常用解析库Beautiful Soup 支持四种解析库，我们推荐使用lxml（速度快，容错能力强，可以解析html 和 xml)。话不多说上代码。 如果还未安装的话，可以命令行输入$ pip install beautifulsoup4 from bs4 import BeautifulSoup import re html = """ &lt;html>&lt;head>&lt;title>The Dormouse's story&lt;/title>&lt;/head> &lt;body> &lt;p class="title" name="dromouse">&lt;b>The Dormouse's story&lt;/b>&lt;/p> &lt;p class="story">Once upon a time there were three little sisters; and their names were &lt;a href="http://example.com/elsie" class="sister" id="link1">&lt;!-- Elsie -->&lt;/a>, &lt;a href="http://example.com/lacie" class="sister" id="link2">Lacie&lt;/a> and &lt;a href="http://example.com/tillie" class="sister" id="link3">Tillie&lt;/a>; and they lived at the bottom of a well.&lt;/p> &lt;p class="story">...&lt;/p> """ soup = BeautifulSoup(html,'lxml') #也可以不适用lxml 库使用默认的解析库 soup = BeautifulSoup(html) #推荐使用lxml 解析 #尝试输出一下 print(soup.prettify()) 发现结果如下，我们的html代码被调整好了格式,不再是那么一坨 &lt;html> &lt;head> &lt;title> The Dormouse's story &lt;/title> &lt;/head> &lt;body> &lt;p class="title" name="dromouse"> &lt;b> The Dormouse's story &lt;/b> &lt;/p> &lt;p class="story"> Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1"> &lt;!-- Elsie --> &lt;/a> , &lt;a class="sister" href="http://example.com/lacie" id="link2"> Lacie &lt;/a> and &lt;a class="sister" href="http://example.com/tillie" id="link3"> Tillie &lt;/a> ; and they lived at the bottom of a well. &lt;/p> &lt;p class="story"> ... &lt;/p> &lt;/body> &lt;/html> prettify()方法这就是我们要介绍的第一个方法 prettify(). 这个方法将需要解析的字符串以标准的格式缩进格式输出，有一点需要注意一下，如果我们原来的字符串没有闭合的话(比如缺少)经过这个调用会自动更正格式，补全代码（但这一步不是这个函数做的，而是进行初始化时就已经自动完成。）我们学习解析库肯定是要学如何提取信息的啊，那么怎么办呢： 节点选择器-栗子 html 同上面的 print(soup.title) print(soup.title.string) print(soup.p) print(type(soup.title)) >>> &lt;title>The Dormouse's story&lt;/title> The Dormouse's story &lt;p class="title" name="dromouse">&lt;b>The Dormouse's story&lt;/b>&lt;/p> &lt;class 'bs4.element.Tag'> 很直观的看到我们可以通过节点选择器简单的提取信息，选择器解析到的是Tag类型，其中的string属性可以得到节点的文本内容。那么只能提取到文本内容吗，显然不是。 >>>列举一下信息提取方式 print(soup.title.name) #获取节点的名称 print(soup.p.attrs) #可以获取该节点的所有属性，比如id class name等等，它返回一些字典型数据，假如我们只想要其中一个属性那么可以这样 print(soup.p.attrs['name']) 如果你觉得这样很烦，那么可以更直接一点 print(soup.p['name']) ****划重点了 提取出的属性值不一定唯一，name值一般只有一个所以输出为文本类型的，但同一标签里会有很多个class属性，因此class属性提取结果为列表型。 每一步提取出来的节点都是'bs4.element.Tag类型的，也就意味着我们可以二次提取，嵌套操作！ >>> print(soup.head.title) >>>&lt;title>The Dormouse's story&lt;/title> 这一步我们先提取出head 节点，然后再次调用节点选择器，提取出title. 关联选择。网页结构并不是我们理想的那种，有时候我们需要多次选择，从一个基准出发然后选择他的子节点，父节点，兄弟节点（没有姐妹节点）。1——选择节点元素后，如果想直接获得直接子节点我们调用contents属性即可。 （1）子节点 from bs4 import BeautifulSoup html = 原来的html 去除第一个p标签。 soup = BeautifulSoup(html,'lxml') print(soup.p.contents) >>> ['Once upon a time there were three little sisters; and their names were\n', &lt;a class="sister" href="http://example.com/elsie" id="link1">&lt;!-- Elsie -->&lt;/a>, ',\n', &lt;a class="sister" href="http://example.com/lacie" id="link2">Lacie&lt;/a>, ' and\n', &lt;a class="sister" href="http://example.com/tillie" id="link3">Tillie&lt;/a>, ';\nand they lived at the bottom of a well.'] # 可以看出返回结果为列表形式，这里列举的全是直接子节点，假如这些节点内部还有节点（孙节点），也不会单独列出来而是包含在对应的父节点里。 不知你有没有想过为什么叫子节点，是不是有个children属性，答案是肯定的，真的有children属性，我们也可以通过调用它来提取子节点。 >>> print(soup.p.children) >>> &lt;list_iterator object at 0x00000252F0878BE0> 这是一个迭代器，我们可以通过遍历提取出所有的内容。也可以通过list将其转化为列表（但没必要这么做） >>> for i in soup.p.children: print(i) >>> Once upon a time there were three little sisters; and their names were &lt;a class="sister" href="http://example.com/elsie" id="link1">&lt;!-- Elsie -->&lt;/a> , &lt;a class="sister" href="http://example.com/lacie" id="link2">Lacie&lt;/a> and &lt;a class="sister" href="http://example.com/tillie" id="link3">Tillie&lt;/a> ; and they lived at the bottom of a well. （2）父节点，祖先节点。 如果你想把这个节点下的所有节点都列出来（子节点，子孙节点。。。。。）那么你可以调用descendants属性，同样这也是一个迭代器类. 那么父节点相信你们也能猜一下喽，parents属性就可以调用，不在贴代码，自己实践一下吧。既然有子孙节点，那么祖先节点也一定存在（不要问为什么没有爷爷节点）对应的属性为parents. 说完了先辈和后代，当然不能忘记同辈，我们还有兄弟节点需要了解一下。ps____这里不在赘述代码。 （3）兄弟节点 print(soup.p.next_sibling) print(soup.p.prevoius_sibing) print(list(soup.p.prevoius_sibings)) print(list(soup.p.next_siblings)) 分别可以得到你选择节点的下一个兄弟元素，上一个兄弟元素，以及前面所有兄弟元素的生成器，后面所有元素的生成器。 （4）提取信息 前面已经讲了关联节点的选择方法，既然选择到了对应的节点那么如何获取对应的信息呢，我们的目的是提取信息的呀！ html = """ &lt;html> &lt;body> &lt; p class='story'> once upon a time &lt;a href="http://www.baidu.com" class ="sister" id = 'link1'>Bob&lt;/a>&lt;a href='http://127.0.0.1:5000' class ='brother' id = 'link2'>Lacie&lt;/a> &lt;/p> """ soup = BeautifulSoup(j=html,'lxml') print(type(soup.a.next_sibling)) print(soup.a.next_sibling) print(soup.a.next_sibling.string) # 选取的生成器元素 print(list(soup.a.next_siblings)[0]) # 获取特定节点 print(list(soup.a.next_siblings)[0].attr['class']) print(list(soup.a.next_siblings)[0]['class']) #获取节点的class属性 和我们刚开始讲的方法是一样的！ 方法选择器.上面的选择方法是通过属性选择，速度比较快，但明显不够灵活，还好BeautifulSoup 为我们提供了类似于正则表达式的一种查询方法find_all() find()等，输入对应的参数就可以灵活查询。1：find_all() 和正则的findall()功能类似，但需要的不再是正则表达式，而是一些属性或者文本进行查询操作。 name故名思意通过节点的名称来查询 它的标准形式是find_all(name,attrs,recursive,text,**kwargs) html = """ &lt;div class = 'A'> &lt;div class = 'A_B'> &lt;h4> hello &lt;/h4> &lt;/div> &lt;div class="body"> &lt;ul class='list' id = 'list-1'> &lt;li class='element'>A&lt;/li> &lt;li class='element'>B&lt;/li> &lt;li class='element'>C&lt;/li> &lt;/ul> &lt;ul class='list list-small' id = 'list-2'> &lt;li class='element'>A&lt;/li> &lt;li class='element'>B&lt;/li> &lt;/ul> &lt;/div> &lt;/div> """ soup = BeautifulSoup(html,'lxml') print(soup.find_all(name = 'ul')) print(type(soup.find_all(name='ul')[0])) >>> [&lt;ul class="list" id="list-1"> &lt;li class="element">A&lt;/li> &lt;li class="element">B&lt;/li> &lt;li class="element">C&lt;/li> &lt;/ul>, &lt;ul class="list list-small" id="list-2"> &lt;li class="element">A&lt;/li> &lt;li class="element">B&lt;/li> &lt;/ul>] &lt;class 'bs4.element.Tag'> 通过返回结果我们可以清晰的看到返回的是列表形式，其中每一个均为bs4.element.Tag类型。 既然是这种类型那么显然我们可以嵌套查询 for ul in soup.find_all(name='ul'): print(ul.find_all(name='li')) >>> [&lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>, &lt;li class="element">C&lt;/li>] [&lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>] 那么我们要获取每个li节点的信息也就顺理成章的可以写成 for ul in soup.find_all(name='ul'): for li in ul.find_all(name = 'li'): print(li.string) attrs我们也可以输入属性查询例如： print(soup.find_all(attrs={'id': 'list-1'})) 该参数为字典类型attrs={'**': "***"} 对于一些常用的属性比如id class 我们可以不用attrs传递（可能是因为懒吧）我们可以这么做： print(soup.find_all(id = 'list-1')) print(soup.find_all(class_= 'element')) >>> [&lt;ul class="list" id="list-1"> &lt;li class="element">A&lt;/li> &lt;li class="element">B&lt;/li> &lt;li class="element">C&lt;/li> &lt;/ul>] [&lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>, &lt;li class="element">C&lt;/li>, &lt;li class="element">A&lt;/li>, &lt;li class="element">B&lt;/li>] 可能有人要问为什么class要加下划线，因为class在Python里是关键字，我们这里需要区分一下。 text匹配节点的文本，传入的可以是字符串，或者正则表达式（有点抢饭碗的意思） import re print(soup.find_all(text = re.compile('A'))) print(soup.find_all(text ='A')) 返回所有匹配正则表达式的节点文本组成的列表。 find()返回单个元素，其他用法和find_all()相同还有一些方法，比如说 find_parents() /find_parent() #返回祖先节点 /返回父节点 find_next_siblings() / find_next_siblings #返回后面所有兄弟节点/返回后面的第一个兄弟节点 find_previous_siblings() / find_previous_sibling() #返回前面所有兄弟节点/返回前面第一个兄弟节点 find_all_next / find_next #返回节点后所有符合条件的节点/ 返回第一个 find_all_previous() / find_previous() #返回节点前所有符合条件的节点/ 返回第一个 CSS选择器如果对web熟悉的话那么CSS选择器你肯定很熟悉，不熟悉的话可以找一下教程。使用CSS选择器我们需要调用select()方法，传入相应的CSS选择器即可 print(soup.select('.A .A_B')) print(soup.select('ul li')) print(soup.select("#list-1 .element")) # 支持嵌套选择 for ul in soup.select('ul'): print(ul.select('li')) # 获取属性方法同上面所讲不在赘述 # 获取文本多加了一个方法get_text() for li in soup.select('li'): print(li.get_text()) print(li.string) o文明k，大概就这么多东西。推荐lxml库，节点选择器速度快但是很繁琐。建议使用find_all(),如果对CSS选择器熟悉的话，推荐select()方法选择。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests 库 and 正则表达式]]></title>
    <url>%2F2018%2F09%2F21%2Fclass1%2F</url>
    <content type="text"><![CDATA[本文简单的讲一下requests库的基本操作 requests库的安装首先确定你已经安装了requests库，如果没有的话 pip install requests 简单的使用请求可以这样写 $ r = requests.get("http://www.baidu.com") #对应GET $ data = {'key':'value'} $ r = requests.post("http://www.baidu.com",data=data) #对应POST 提交表单, data为表单数据，以字典型数据存储 $ #主要前两种后面几乎用不到 $ r = requests.put("http://www.baidu.com") #对应PUT $ r = requests.delete("http://www.baidu.com") #对应删除DELETE $ r = requests.head("http://www.baidu.com") # 很方便的看出请求很简单，一行代码就可以完成请求，但是这只是一般网站的主页，我们肯定是要去其他部分提取信息的，所以会附加一大串的参数，比如 r = requests.get('http://httpbin.org/get?name=germey&amp;age=22') 构造一些参数虽然也能完成任务，但翻页操作什么的每次都构造这个显然是不够好用，这个库当然不会让我们用这么笨的方法 for age in range(1,50): params = { "name" : "germey", "age" : age } r = requests.get('http://httpbin.org/get',params = params) 成功完成附加参数的提交 我们还可能遇到一个最low的反爬措施（知乎） 通过User-Agent 浏览器标识来反爬虫，这个我们可以构造headers轻易的破解，还有一些需要登陆才能操作的网站同样也可以通过构造headers headers = { 'Cookie' : 'ABTEST=0|1534834504|v1; SNUID=1C3DF36DB3B6C05E33DD8DD8B303CE03; IPLOC=CN3201; SUID=AF8E41DF3E18960A000000005B7BB748; SUID=AF8E41DF2C18960A000000005B7BB748; SUV=00151DF1DF418EAF5B7BB74C32B4D814; weixinIndexVisited=1; ppinf=5|1534908615|1536118215|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZTo2OlBzeWNob3xjcnQ6MTA6MTUzNDkwODYxNXxyZWZuaWNrOjY6UHN5Y2hvfHVzZXJpZDo0NDpvOXQybHVJV0FROGwzSTFjYlg3M3Z1akxfd3prQHdlaXhpbi5zb2h1LmNvbXw; pprdig=xm9mkmMYDlvMRbH0pAjDEpcsEDCvoz3ORcB-9-lzvVhkxyM55AmN7NQJ8KU3Ei67B6DAqmo_DjyIu3NchvKgznUCthv3eMG2u_T1MhiMkJD7nV3HrDRKv0KVeNyQnt4Zl6D4y1v8SlHfHd-6aGhSCKW_NDIv_JqJmP-7eWQgsKw; sgid=22-36710215-AVt82MciatqU4SaMibG2iceyUs; sct=3; ppmdig=1534925103000000f514b9c5b91510b7cd851626696ec141; JSESSIONID=aaaGoZeH3bNXx-s9OFBvw; SL_GWPT_Show_Hide_tmp=1; SL_wptGlobTipTmp=1', 'Host' : 'weixin.sogou.com', 'Upgrade-Insecure-Requests' : '1', 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36' } requests,get('http://weixin.sogou.com',headers = headers) 当然也可以cookies = { 'a' : 'a', } requests,get('http://weixin.sogou.com',cookies = cookies) 但显然直接复制更方便 如果遇到反爬措施怎么更换代理呢，requests也有十分简单的方法，类似于上面的参数提交 proxies = { "http" : "http:127.0.0.1:8080", "https" : "https://127.0.0.0:5555" } r = requests.get('http:www.zhihu.com',proxies=proxies) #当然上面的代理是不能用的，你可以找国内哪些免费网站上寻找，或者用后续的维护一个代理池，当然付费代理最为好用。 还有一个超时操作的处理直接在末尾的timeout 参数赋值即可，如果你使用的是GET、OPTIONS、POST、PUT、PATCH 或者 DELETE，那么你可以通过 allow_redirects 参数禁用重定向处理： r = requests.get('http://github.com', allow_redirects=False) 常遇到的问题遇到网络问题（如：DNS 查询失败、拒绝连接等）时，Requests 会抛出一个 ConnectionError 异常。 如果 HTTP 请求返回了不成功的状态码， Response.raise_for_status() 会抛出一个 HTTPError 异常。 若请求超时，则抛出一个 Timeout 异常。 若请求超过了设定的最大重定向次数，则会抛出一个 TooManyRedirects 异常。 所有Requests显式抛出的异常都继承自 requests.exceptions.RequestException 。我们通过上面的设置，利用try except 函数可以容易的捕获这些错误此外还有一些证书认证操作，比如12306,他们的证书没有被官方认证，所以爬取会报错，这个设置也很简单。把 verify 参数设置为False 即可，不过会提醒我们一个警告，我们可以选择忽略他， import requests from requests.packages import urllib3 urllib3.disable_warnings() response = requests.get('https//www.12306.cn',verify=False) 讲完了请求那么我们获取的数据怎么拿到呢。 r.status_code #获取响应码，判断自己是否成功访问网站 r.text #将源码转换为text格式 r.encoding = 'utf-8' # 改变网页源码的编码格式，一般是gbk,或者utf-8 r.url #获取请求的网址 type(**) #输出内容的格式，比如text,json ,dict r.json() #可以发现，调用json(）方法，就可以将返回结果是 JSON 格式的字符串转化为字典dict r.content #图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式， 我们才可以看到这些形形色色的多媒体 所以，想要抓取它们，就要拿到它们的二进制码 正则re正则表达式是处理字符串的强大工具，它有向己特定的语法结构，有了它，实现字符串的检索、替换、匹配验证都不在话下当然,对于爬虫来说，有了它，从HTML 里提取想要的信息就非常方便了可以在开源中国里练手，强大的在线匹配功能http://tool.oschina.net/regex/正则不是python独有的，其他编程语言中也有，re库是整个正则表达式的实现，很方便实用。 match()这个函数可以检测你的正则表达式是否能成功匹配数据，如果失败就返回None,代表匹配失败。举个栗子！ re.match(pattern, string, flags=0) #对应正则表达式，字符串，标识符 栗子： import re str = "Hello 123 456 World is a demo" print(len(str)) result = re.match('^Hello\s\d\d\d\s\d{3}\s\w{5}',str) print(result) print(result.span()) print(result.group()) 我们的正则表达式以^作为开头，然后\s匹配空白字符，\d匹配数字,\d{3}代表匹配三个数字，\w{5},匹配字母数字或下划线共五个。我们运行一下发现匹配到了 Hello 123 456 World，调用group()获取匹配内容，span()获取匹配范围。match()函数有两个参数，1为正则，二为待匹配字符串。如果我们想要得到某一具体内容怎么办呢，比如只想得到123。那么我们可以在正则表达式中用()把它括起来 result = re.match('^Hello\s(\d+)\s\d{3}\sWorld',str) print(result.group(1)) 这样难免有点繁琐，我们使用通用匹配就很方便——.(.可匹配除换行符之外的任意字符。匹配无限次)完成上面的匹配，我们可以这样写 result = re.match('^Hello.*demo',str) print(result.group()) 那么这样真的没有缺点吗，答案是否定的：比如你想得到123这个数字 result = re.match('^Hello.*(\d+).*demo'，str) print(result.group(1)) 运行一下发现结果不如人意并不是我们想要的 这里就涉及到贪婪和非贪婪的问题，.会匹配尽可能多的字符（贪婪匹配），我们只需要简单的更改就能成功(加？).? 是非贪婪匹配，匹配尽可能少的字符，到数字的前面时就默认停止匹配，从而让我们得到正确的结果但这里需要注意，如果匹配的结果在字符串结尾，.*?就有可能匹配不到任何内容了，因为它匹配尽可能少的字符 例如： import re content = 'http://weibo.com/comment/KEraCN' result1 = re.match('http.*?comment/(.*?)',content) result2 = re.match('http.*?comment/(.*)',content) print("result1",result1.group(1)) print('result2', result2.group(1)) >>> result1 result2 KEraCN 修饰符正则表达式可以包含一些可选标志修饰符来控制匹配的模式 修饰符被指定为一个可选的标志。我们改一下字符串 str = "Hello 123 456 World is a demo" result = re.match('^Hello.*demo',str) print(result.group()) 发现报错，未成功得到字符 这是因为．匹配的是除换行符之外的任意字符，当遇到换行符时，.*?就不能匹配了，所以导致匹配失败这里只需加一个修饰符 re.S ，即可修正这个错误： result = re.match('^Hello.*demo',str,re.S) re.I #使匹配大小写不敏感 re.S #使.匹配包括换行符在内的所有字符 还有其他不常用，自己去查阅吧 那么如果我们要匹配的字符包含.或者其他的特殊字符怎么办呢，比如匹配网址http://www.baidu.com。 这个时候就要用到强大的转义字符了 content = 'www.baidu.com' result = re.match('www\.baidu\.com',content) print(result) search()match()方法是从字符串的开头匹配，我们用它来提取信息显然不方便，更适合用来检测某个字符串是否符合规则，所以我们介绍search()-扫描整个字符串返回第一个成功匹配的结果，没找到则返回None. re.search(pattern, string, flags=0) #对应正则表达式，字符串，标识符 栗子： import re html = '''&lt;div id = 'song-list'> &lt;h2 class = 'title'>经典老歌&lt;/h2> &lt;p class ='introduction'> 经典老歌列表&lt;/p> &lt;ul id ='list' class = 'list-gruop'> &lt;li data-view ='2'>一路有你&lt;/li> &lt;li data-view='7'> &lt;a href='/2.mp3' singer='任贤齐'>沧海一声笑&lt;/a> &lt;/li> &lt;li data-view = '4' class ='active'> &lt;a href='/3.mp3' singer='齐秦'>往事随风&lt;/a> &lt;/li> &lt;li data-view='6'>&lt;a href='/4.mp3' singer = 'beyond'>光辉岁月&lt;/a>&lt;/li> &lt;li data-view='5'>&lt;a href='/5.mp3' singer = '陈慧琳'>记事本&lt;/a>&lt;/li> &lt;li data-view='5' &lt;a href ='/6.mp3' singer='邓丽君'>但愿人长久&lt;/a> &lt;/li> &lt;/ul> &lt;/div>''' result = re,search('&lt;li.*?active.*?singer='(.*?)'>(.*?)&lt;/a>',html,re.S) print(result.group(1),result.group(2)) >>>齐秦往事随风 result = re,search('&lt;li.*?singer='(.*?)'>(.*?)&lt;/a>',html,re.S) print(result.group(1),result.group(2)) >>>任贤齐沧海一声笑 假如我们去掉换行符re.S result = re,search('&lt;li.*?singer='(.*?)'>(.*?)&lt;/a>',html) print(result.group(1),result.group(2)) >>>beyond光辉岁月 findall()search方法只能返回一个值，显然不足以满足我们的需求，findall方法返回所有符合要求的字符串，以列表的形式存储。 re.findall(pattern, string, flags=0) html 同上 results = re.findall('&lt;li.*?href='(.*?)'.*?singer='(.*?)'>(.*?)&lt;/a>',html,re.S) for result in results: print(result) print(result[0],result[1],result[2]) >>>自己运行吧 sub()除了提取信息我们还可用用正则修改文本，这时候就用到了sub()方法。还以上面的为例 >>>html = re.sub('&lt;a.*?>|&lt;/a>','',html) results = re.findall('&lt;li.*?>(.*?)&lt;/li>',html,re.S) for result in results: print(result.strip()) >>>自己运行吧 compile()前面所讲的方法都是用来处理字符串的方法，最后再介绍一下 compile()方法，这个方法可以将正则字符串编译成正则表达式对象，以便在后面的匹配中复用 import re contnet = '2018-2-11 12:00' pattrtn = re.compile('\d{2}:\d{2}') 另外， compile()还可以传入修饰符，例如 re.S等修饰符，这样search(),findall()等方法中就不需要额外传了 所以compile()方法可以说是给正则表达式做了一层封装，以使我们更好地复用 ok 扯淡完毕本节结束具体看实例吧]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反爬微信搜狗文章]]></title>
    <url>%2F2018%2F09%2F01%2Fweixin%2F</url>
    <content type="text"><![CDATA[本文涉及到反爬虫措施和代理池 需要用的知识代理池，Pyquery解析库，requests请求库,and 万年不变的MongoDB数据库. 完成这个爬虫我们需要做下列准备，首先当然是分析一下网页的结构，我们可以随便搜索一下，发现如果不登录可以看前十页内容，登陆后可以看到100页，而且每页出现一堆文章列表，打开开发者模式发现他们都各自包含一个超链接，指向微信文章。原来搜狗已经用自己的爬虫爬取了一次文章，然后打包为自己的搜索内容，然而我们多次刷新后会出现验证界面，也就是搜狗的反爬虫机制，这也是我们开头说要用代理的原因，原理基本就是这样，然后贴代码。 模拟登陆首先是设置heads,也就是模拟登陆，网站采取cookies来验证是否登陆，因此我们只需把这些复制过去， headers = { 'Cookie' : 'ABTEST=0|1534834504|v1; SNUID=1C3DF36DB3B6C05E33DD8DD8B303CE03; IPLOC=CN3201; SUID=AF8E41DF3E18960A000000005B7BB748; SUID=AF8E41DF2C18960A000000005B7BB748; SUV=00151DF1DF418EAF5B7BB74C32B4D814; weixinIndexVisited=1; ppinf=5|1534908615|1536118215|dHJ1c3Q6MToxfGNsaWVudGlkOjQ6MjAxN3x1bmlxbmFtZTo2OlBzeWNob3xjcnQ6MTA6MTUzNDkwODYxNXxyZWZuaWNrOjY6UHN5Y2hvfHVzZXJpZDo0NDpvOXQybHVJV0FROGwzSTFjYlg3M3Z1akxfd3prQHdlaXhpbi5zb2h1LmNvbXw; pprdig=xm9mkmMYDlvMRbH0pAjDEpcsEDCvoz3ORcB-9-lzvVhkxyM55AmN7NQJ8KU3Ei67B6DAqmo_DjyIu3NchvKgznUCthv3eMG2u_T1MhiMkJD7nV3HrDRKv0KVeNyQnt4Zl6D4y1v8SlHfHd-6aGhSCKW_NDIv_JqJmP-7eWQgsKw; sgid=22-36710215-AVt82MciatqU4SaMibG2iceyUs; sct=3; ppmdig=1534925103000000f514b9c5b91510b7cd851626696ec141; JSESSIONID=aaaGoZeH3bNXx-s9OFBvw; SL_GWPT_Show_Hide_tmp=1; SL_wptGlobTipTmp=1', 'Host' : 'weixin.sogou.com', 'Upgrade-Insecure-Requests' : '1', 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36' } 请求网页当然cookies会过期复制这个是没用的。然后就是请求网页。 $ global PROXY try: if PROXY: proxies = { 'http' : 'http://' + PROXY } response = requests.get(url = url, allow_redirects = False, headers = headers , proxies = proxies, timeout = 1) else: response = requests.get(url = url, allow_redirects = False, headers = headers, timeout = 1) if response.status_code == 200: return response.text if response.status_code == 302: print("302") proxy = get_proxy() if proxy: print('Using Proxy', proxy) return get_html(url) else: print("Get Proxy Failed") return None except ConnectionError as e: print("Error Occurred", e.args) proxy = get_proxy() return get_html(url,count) except Timeout as T: print("Error Occurred", T.args) proxy = get_proxy() return get_html(url, count) 反爬虫措施触发这里我们获取搜狗的搜索结果页面，如果正常的话返回网页源码，如果触发反爬虫机制，那么我们就选择更换代理，当然我们可以加一个最大重复次数的验证，避免死循环。 def parse_index(html): doc = pq(html) items = doc('.news-box .news-list li .txt-box h3 a').items() for item in items: yield item.attr('href') 我们解析这个网页，然后提取出对应的href，超链接然后通过生成器yield将结果返回上层，再之后就是普通的挨个访问，然后提取信息，这里不在赘述。有一个问题就是pyquery好像不能把时间那个标签完整的提取出来，大佬们可以采取正则表达式尝试一下 from lxml.etree import XMLSyntaxError 在解析过程中，我们发现程序报错显示XMLSyntaxError，这是一个不常见的解析错误，我们将其引入即可，数据库操作还是老样子也不在多提，代理池的事情下篇文章再讲~~~~ 更新一下，搜狗的微信接口貌似加了点料，不能直接复制我们的请求头的全部内容到headers里 不然会永久302 无休止的更换代理也不能得到正确结果，经过测试只复制cookies就可以完成我们的任务。]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu安装Gogs的一些坑和心得]]></title>
    <url>%2F2018%2F08%2F30%2Fhello-world%2F</url>
    <content type="text"><![CDATA[本文是自己准备搭建自己的git仓库时遇到的一些事 为什么选择GogsGogs作为用go语言写的轻量级的git仓库很适合大家部署在自己的服务器上，建立私人仓库更是很舒服（觉得自己代码不够漂亮）废话不多说进入正题。 需要的基本内容首先我们要安装数据库：所有的版本都支持 MySQL、PostgreSQL、MSSQL 和 TiDB（使用 MySQL 协议）作为数据库，并且均使用构建标签（build tags）cert 进行构建。需要注意的是，不同的版本的支持状态有所不同，请根据实际的 Gogs 提示进行操作。。我选择的是mysql 代码很简单——sudo apt-get install mysql-server 根据提示一步步安装，完成后可以终端输入 mysql --version 成功的话会看到mysql的版本号 第二步安装git--sudo apt-get install git 同样根据提示进行安装或者 sudo apt-get -y install git （默认所有需要输入操作输入yes） git --version查看版本号观察是否安装成功 上面都成功之后开始创建一个新用户git （是系统用户和git软件并无联系）我们将gogs安装在git用户中 sudo adduser git 按照提示设置密码等 切换git用户 sudo su - git 先安装go语言，linux可直接用sudo apt-get install golang (直接打go是没有的会显示找不到软件包，go又名golang,镜像源中go的包为golang) 附注：如出现找不到软件包可以尝试更新镜像源，国内较好的镜像源有阿里云等，我们可以通过 software-properties-gtk 然后选择合适的镜像源也可以通过编辑源文件——vim /etc/apt/sources.list 亦可以sudo gedit /etc/apt/source.list 然后把自己网上找的镜像源代码复制进去 比如这个网站里的一个源http://blog.sina.com.cn/s/blog_6bc5571a0101077t.html 还可以通过wget 命令直接从网上下载压缩包例如 su - git wget https://dl.google.com/go/go1.9.2.linux-amd64.tar.gz 网址可以自己在网页上寻找最新版的压缩包 解压并删除该安装包（建议先不删避免出现错误之后重新下载） tar -xf go1.9.2.linux-amd64.tar.gz rm -f go1.9.2.linux-amd64.tar.gz tar.gz为二进制压缩文件解压后相当于安装在了该计算机上 我们需要设置环境变量设置 GOROOT 和 GOPATH 目录到系统环境，这样，我们就可以在 git 用户下执行 go 命令。 执行下方的命令 cd ~/ echo 'export GOROOT=$HOME/local/go' >> $HOME/.bashrc echo 'export GOPATH=$HOME/go' >> $HOME/.bashrc echo 'export PATH=$PATH:$GOROOT/bin:$GOPATH/bin' >> $HOME/.bashrc 然后运行source ~/.bashrc 重载Bash 一定确保自己不会写错！认真认真认真！！！！！重要的事情说三遍 大体上完成了我们可以进入正题了！！！！ 开始安装gogs使用git安装gogs—–即使用git用户用go命令下载Gogs $ su - git $ go get -u github.com/gogits/gogs 此命令将在 GOPATH/src 目录下载 Gogs 的所有源代码。 个人觉得这个非常慢，刚开始弄甚至以为电脑坏了。。 切换至 $GOPATH/src/github.com/gogits/gogs 目录，并且使用下列命令搭建 Gogs。 cd $GOPATH/src/github.com/gogits/gogs go build 官网上还可以构造分支什么的可以尝试一下。。。不做论述 既然这个比较慢那么当然有快捷办法——参考安装go的方法我们可以运行wget wget -c https://dl.gogs.io/0.11.19/linux_amd64.tar.gz 等待下载完成tar zxvf 压缩包名即可将二进制的文件安装进行下列操作 两种方法完成后都可以进入gogs文件夹运行 ./gogs web 看到这里恭喜你，基本完成了。我们可以crtl c让进程停下。。因为我们的数据库还没有配置 配置数据库mysql -u root(这是用户名） -p （因为我们要创建用户所以要用最高权限的用户） 输入密码进入然后然后数据库的基本操作百度吧。。（数据库名和数据库用户名不是一个概念） exit; 退出数据库 再次进入gogs文件夹 ./gogs web 浏览器输入对应网址。。。。。不知道本机ip?那么 ipconfig -a 即可看到自己的ip 我的是：http://192.168.176.129:3000/然后设置对应的配置，如果还想要其他的操作那么去这里https://linux.cn/article-9391-1.html 以及一些解压文件https://blog.csdn.net/zhongnanjun_3/article/details/6312678 还有vim编译器的读写操作https://my.oschina.net/liujinofhome/blog/36648 同时因为电脑的不同的不同可能还会有其他各种奇葩错误，且行且珍惜]]></content>
      <categories>
        <category>ubuntu</category>
      </categories>
      <tags>
        <tag>ubuntu</tag>
        <tag>gogs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[饿了么爬虫尝试]]></title>
    <url>%2F2018%2F08%2F29%2Felm%2F</url>
    <content type="text"><![CDATA[本文是自己临时起意想写的一点东西。 需要用到的知识突发奇想，想爬取外卖商家的信息（虽然吃了一暑假外卖，要吐的感觉）。用到了分析Ajax请求，储存库依旧是MongoDB，还用了一下代理池（因为第一次爬取出现了429状态码，限制爬取速度，但是配置完代理池之后这个错误竟然消失了） headers = { "cookie": "ubt_ssid=rp2q5djs0u6ewmuztcx8mgd2mrkv0vgy_2018-08-29; _utrace=04d8824d7caede2317d2342af2517ebf_2018-08-29; SL_GWPT_Show_Hide_tmp=1; SL_wptGlobTipTmp=1; track_id=1535537453|22aa0399855feb682fdaec9615fcab851e46acb633c7ede4a1|28e66234a33c368946d7476ac4a530f8; USERID=457663306; SID=8qvMlmIw0lzs0oBSvJXJoHiG0ohT7yC77chA", "referer": "https://www.ele.me/place/wtswdqsgbhhq?latitude=32.11645&amp;longitude=118.933531", "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36", "x-shard": "loc=118.933531,32.1164" } cookies因为需要登陆后才能看到商家信息，所以我们在heads中加入cookies，User-agent参数则是屏蔽爬虫的标识让网站认为我们是正常的浏览。通过观察我们发现网站采取Ajax,和微信与头条一样，下拉能看到更多的内容，因此我们直接爬去这部分字符串即可 BASE_URL = 'https://www.ele.me/restapi/shopping/restaurants?' params = { "extras[]": "activities", "geohash": "wtswdqsgbhhq", "latitude": 32.11645, "limit": 24, "longitude": 118.933531, "offset": offset, "terminal": "web" } url = BASE_URL + urlencode(params) 构造url通过urlencode将文本拼接为正常的url 参数我们可以直接打印出来然后访问这个url 发现返回的是一堆字符串，当然我们要的信息就在里面 response = requests.get(url=url, headers=headers,timeout = 1) if response.status_code == 200: return response.json() timeout 为超时处理，如果超过指定时间未响应则报错得到字符串列表之后通过查找则可以找到相关的信息 for item in json: yield { "shopname" : item.get('name'), "adress" : item.get('address'), "opening_hours" : item.get('opening_hours'), "flavors" : item.get('flavors'), "piecewise_agent_fee" : item.get('piecewise_agent_fee').get('description'), "order_lead_time" : item.get('order_lead_time'), } 代理的使用 分别为店家名称，地址，开放时间，特色，配送费，平均配送时间。 之后储存全套服务不在啰嗦。 还剩下刚开始说的代理池，虽然没太大用，但为了避免时不时的429状态码，还是加上为妙。 def get_proxy(): try: response = requests.get(PROXY_POOL_URL) if response.status_code == 200: return response.text return None except ConnectionError: return None 运用代理则为 if PROXY: proxies = { "https" : "https://" + PROXY } print("Using proxy",PROXY) response = requests.get(url = url,headers = headers,allow_redirects = False,proxies = proxies,timeout = 1) proxies 就是requests包中的代理参数，allow_redirects是禁止重定向，requests函数遇到错误会自动帮我们重定向，因为我们要用代理所以禁止重定向。 基本这个爬虫就完成了，说说它的缺点：这个界面没有详细的餐点目录仅有一些店家的信息，还需多多努力，且行且珍惜]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单的爬虫]]></title>
    <url>%2F2018%2F08%2F29%2Fpyquery%2F</url>
    <content type="text"><![CDATA[本文以简单的例子讲如何写爬虫 ##需要的知识多进程，pyquery解析，pymongo链接数据库，requests发送请求 这里以一个简单的实例讲解一下如何提取网页信息并储存在MongoDB数据库 主代码结构爬取的网站是https://isujin.com主函数很简单一层一层的传递 def main(page): urls = get_index(page) for url in urls: html = get_html(url) if html: data = parse_html(html) if data: save_as_text(html) save_to_mongo(data)# 如果没安装mongo可以取消此项 save_picture(html) 获取链接首先是获取所有的链接 def get_index(page): URL = 'https://isujin.com/page/{}'.format(page) try: response = requests.get(URL) if response.status_code == 200: html = response.text doc = pq(html) urls = doc('#primary .post > a').items() for url in urls: url = url.attr('href') yield url return doc('#post0 > h2 > a').attr('href') except ConnectionError: return None 我们提取出所有的网页的url 如果返回值的状态码为200证明为正确响应，通过pq(html)将HTML网页转化为pyquery的格式，pyquery依据CSS选择器提取信息，#开头根据标签的id属性提取；.开头则依据class属性提取 具体CSS选择器的内容可以去http://www.w3school.com.cn/ urls = doc('#primary .post > a').items() 生成器items() 在pyquery中是一个生成器把所有的选中的许多生成一个列表，然后我们用for循环进行遍历每个标签并提取出链接 通过yield迭代器将链接返回到外层 提取信息返回html文档的函数不在赘述，很简单。接下来是解析html文档提取我们要的信息 def parse_html(html): doc = pq(html) title = doc('.title').text() time = doc('div.stuff > span:nth-child(1)').text() readcount = doc('div.stuff > span:nth-child(2)').text() content = doc('.content').text() return { 'title' : title, 'time' : time, 'readcount' : readcount, 'content' : content } 和提取herf一样 通过CSS 选择器获取我们索要的节点然后text() 即可提取所要的文本信息。之后以字典的形式返回数据到外层函数（因为数据库存储是以字典型） mongodb储存信息client = pymongo.MongoClient(MONGO_URl) db = client[MONGO_DB] def save_to_mongo(data): if db[MONGO_TABLE].update({'title' : data['title']},{'$set' : data}, True): print("Saved to Mongo", data['title']) else: print("Saved to Mongo Failed", data['title']) mongodb数据库的操作也极为简单，设定好MONGO_URL = ‘localhost’ MONGO_DB = ‘数据库名称’ MONGO_TABLE = ‘表名’我们采取更新的方式储存，如果信息已经出现我们选择更新，以title为关键字，如果title对应的数据已经出现那么就更新它更新的内容为{‘$set’ : data}，也就是data的全部数据，若最后一个参数为false 那么如果只执行更新操作，原来不存在的数据不会插入. MONGO 学习网站可以参考这个很简单http://www.runoob.com/mongodb/mongodb-update.html 储存照片（content)其他储存照片也是一样的不在赘述贴一下代码 def save_picture(html): doc = pq(html) images = doc('#jg > a').items() if not os.path.exists(doc('title')): os.mkdir(doc('title')) for image in images: image = image.attr('href') try: response = requests.get(image) if response.status_code == 200: file_path = '{0}/{1}.jpg'.format(doc('.title').text(),md5(response.content).hexdigest()) if not os.path.exists(file_path): print("Downloading",file_path) with open(file_path,'wb') as f: f.write(response.content) else: print("Already Downloaded",file_path) except ConnectionError as e: print("Download Error", e.args) 需要注意的是储存文本时f.write()中的内容必须为字符串 多进程一个简易爬虫就写好了，还差最后一个多进程问题 if __name__ == "__main__": pool = Pool() group = ([x for x in range(1,14)]) pool.map(main,group) pool.close() pool.join() pool = Pool() 没有指定的话会根据系统自动分配，通过map()函数,map(main,group)，第一个参数是方法，第二个是数组列表，简单的来说就是把main方法依次执行close() 关闭pool，使其不在接受新的任务。join() 主进程阻塞，等待子进程的退出， join方法要在close或terminate之后使用具体请看大神的作品 https://cuiqingcai.com/3335]]></content>
      <categories>
        <category>Python</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python链接数据库报错]]></title>
    <url>%2F2018%2F08%2F28%2Ffirst%2F</url>
    <content type="text"><![CDATA[本文是自己刚接触python时踩的坑 在pycharm中连接数据库报错$ import hashlib,os,requests,ast $ import pymysql $ from bs4 import BeautifulSoup $ #数据库链接 $ conn=pymysql.connect(host="localhost",port=3306,user="root",passwd="1820401746yuan.A",db="pm") $ cursor=conn.cursor() #创建cursor对象 $ host 主机名也可用127.0.0.1.代替（本地服务器） port端口（附查看方式，登陆mysql输入下列代码） show global variables like 'port'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | port | 3306 | +---------------+-------+ 1 row in set, 1 warning (0.16 sec) user 用户名默认为root——权限最大的用户 ERROR 1045 (28000): Access denied for user 'root'@'localhost' (using password: NO) 网上有很多的办法，但都不能解决我的问题。 比如赋予数据库权限，以及创建新用户等等。。。 然后在我的努力寻找下成功了 最简单的方法更换了root密码的认证方式解决，新版mysql使用的caching_sha2_password，换成mysql_native_password我就可以连上了。 然后输入ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY 'password'; password即你的密码） 转自https://blog.csdn.net/dongweionly/article/details/80273095]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Mysql</tag>
      </tags>
  </entry>
</search>
